{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0,). Analytic: 6.00000, Numeric: 6.00000\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0,). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1,). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0, 0). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (0, 1). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1, 0). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1, 1). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0,). Analytic: 0.57612, Numeric: 0.57612\n",
      "Gradients are not different at (1,). Analytic: -0.78806, Numeric: -0.78806\n",
      "Gradients are not different at (2,). Analytic: 0.21194, Numeric: 0.21194\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: 0.20603, Numeric: 0.20603\n",
      "Gradients are not different at (0, 1). Analytic: 0.56005, Numeric: 0.56005\n",
      "Gradients are not different at (0, 2). Analytic: -0.97212, Numeric: -0.97212\n",
      "Gradients are not different at (0, 3). Analytic: 0.20603, Numeric: 0.20603\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0, 0). Analytic: 0.68145, Numeric: 0.68145\n",
      "Gradients are not different at (0, 1). Analytic: 0.03393, Numeric: 0.03393\n",
      "Gradients are not different at (0, 2). Analytic: 0.03393, Numeric: 0.03393\n",
      "Gradients are not different at (0, 3). Analytic: -0.74931, Numeric: -0.74931\n",
      "Gradients are not different at (1, 0). Analytic: 0.10923, Numeric: 0.10923\n",
      "Gradients are not different at (1, 1). Analytic: 0.29692, Numeric: 0.29692\n",
      "Gradients are not different at (1, 2). Analytic: 0.29692, Numeric: 0.29692\n",
      "Gradients are not different at (1, 3). Analytic: -0.70308, Numeric: -0.70308\n",
      "Gradients are not different at (2, 0). Analytic: 0.15216, Numeric: 0.15216\n",
      "Gradients are not different at (2, 1). Analytic: 0.41362, Numeric: 0.41362\n",
      "Gradients are not different at (2, 2). Analytic: -0.97941, Numeric: -0.97941\n",
      "Gradients are not different at (2, 3). Analytic: 0.41362, Numeric: 0.41362\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)[[2, 1, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: -0.88080, Numeric: -0.88080\n",
      "Gradients are not different at (0, 1). Analytic: 0.88080, Numeric: 0.88080\n",
      "Gradients are not different at (1, 0). Analytic: -0.83337, Numeric: -0.83337\n",
      "Gradients are not different at (1, 1). Analytic: 0.83337, Numeric: 0.83337\n",
      "Gradients are not different at (2, 0). Analytic: 0.92822, Numeric: 0.92822\n",
      "Gradients are not different at (2, 1). Analytic: -0.92822, Numeric: -0.92822\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (0, 1). Analytic: 0.04000, Numeric: 0.04000\n",
      "Gradients are not different at (1, 0). Analytic: -0.02000, Numeric: -0.02000\n",
      "Gradients are not different at (1, 1). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (2, 0). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (2, 1). Analytic: 0.04000, Numeric: 0.04000\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 25455.182912\n",
      "Epoch 1, loss: 26928.310630\n",
      "Epoch 2, loss: 26611.885157\n",
      "Epoch 3, loss: 26995.425987\n",
      "Epoch 4, loss: 27096.880776\n",
      "Epoch 5, loss: 26210.793847\n",
      "Epoch 6, loss: 26911.822848\n",
      "Epoch 7, loss: 25936.854204\n",
      "Epoch 8, loss: 24215.358505\n",
      "Epoch 9, loss: 26642.289629\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1317257ca58>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15\n",
      "Epoch 0, loss: 25994.650562\n",
      "Epoch 1, loss: 26920.096406\n",
      "Epoch 2, loss: 25213.456542\n",
      "Epoch 3, loss: 28049.569628\n",
      "Epoch 4, loss: 26689.940869\n",
      "Epoch 5, loss: 25386.348590\n",
      "Epoch 6, loss: 25039.112087\n",
      "Epoch 7, loss: 27265.390094\n",
      "Epoch 8, loss: 27256.802332\n",
      "Epoch 9, loss: 28062.113944\n",
      "Epoch 10, loss: 26052.925354\n",
      "Epoch 11, loss: 26209.660907\n",
      "Epoch 12, loss: 27484.413573\n",
      "Epoch 13, loss: 27446.307671\n",
      "Epoch 14, loss: 25695.665638\n",
      "Epoch 15, loss: 26972.530221\n",
      "Epoch 16, loss: 25808.123091\n",
      "Epoch 17, loss: 25296.899747\n",
      "Epoch 18, loss: 27734.807150\n",
      "Epoch 19, loss: 27202.949737\n",
      "Epoch 20, loss: 25885.015066\n",
      "Epoch 21, loss: 26735.523605\n",
      "Epoch 22, loss: 26820.567161\n",
      "Epoch 23, loss: 25807.412469\n",
      "Epoch 24, loss: 26162.520955\n",
      "Epoch 25, loss: 26714.836530\n",
      "Epoch 26, loss: 27056.775451\n",
      "Epoch 27, loss: 25838.665197\n",
      "Epoch 28, loss: 27364.895207\n",
      "Epoch 29, loss: 25736.090354\n",
      "Epoch 30, loss: 27222.549582\n",
      "Epoch 31, loss: 26052.654146\n",
      "Epoch 32, loss: 26390.551944\n",
      "Epoch 33, loss: 28193.099629\n",
      "Epoch 34, loss: 25491.865036\n",
      "Epoch 35, loss: 27020.827638\n",
      "Epoch 36, loss: 26977.541775\n",
      "Epoch 37, loss: 26964.163076\n",
      "Epoch 38, loss: 27282.343140\n",
      "Epoch 39, loss: 26369.870117\n",
      "Epoch 40, loss: 25530.419421\n",
      "Epoch 41, loss: 27060.582040\n",
      "Epoch 42, loss: 26776.341415\n",
      "Epoch 43, loss: 25614.341673\n",
      "Epoch 44, loss: 27358.130328\n",
      "Epoch 45, loss: 25605.823761\n",
      "Epoch 46, loss: 27810.900971\n",
      "Epoch 47, loss: 26493.566082\n",
      "Epoch 48, loss: 26388.393620\n",
      "Epoch 49, loss: 25520.294039\n",
      "Epoch 50, loss: 27064.804272\n",
      "Epoch 51, loss: 26968.221314\n",
      "Epoch 52, loss: 26954.209908\n",
      "Epoch 53, loss: 24802.844362\n",
      "Epoch 54, loss: 26525.732444\n",
      "Epoch 55, loss: 28438.182437\n",
      "Epoch 56, loss: 26451.868339\n",
      "Epoch 57, loss: 26486.399173\n",
      "Epoch 58, loss: 26336.782827\n",
      "Epoch 59, loss: 27071.340968\n",
      "Epoch 60, loss: 26172.600321\n",
      "Epoch 61, loss: 26981.620996\n",
      "Epoch 62, loss: 26127.787349\n",
      "Epoch 63, loss: 27188.068511\n",
      "Epoch 64, loss: 27012.378910\n",
      "Epoch 65, loss: 28477.654860\n",
      "Epoch 66, loss: 26764.645559\n",
      "Epoch 67, loss: 27648.616371\n",
      "Epoch 68, loss: 26687.730673\n",
      "Epoch 69, loss: 27000.891124\n",
      "Epoch 70, loss: 28418.918113\n",
      "Epoch 71, loss: 25861.563560\n",
      "Epoch 72, loss: 27630.810500\n",
      "Epoch 73, loss: 26628.952418\n",
      "Epoch 74, loss: 25846.855812\n",
      "Epoch 75, loss: 27567.532691\n",
      "Epoch 76, loss: 25647.270618\n",
      "Epoch 77, loss: 28231.170722\n",
      "Epoch 78, loss: 25472.211083\n",
      "Epoch 79, loss: 26639.484144\n",
      "Epoch 80, loss: 27040.179769\n",
      "Epoch 81, loss: 27023.111153\n",
      "Epoch 82, loss: 26397.094174\n",
      "Epoch 83, loss: 26878.474041\n",
      "Epoch 84, loss: 26307.892750\n",
      "Epoch 85, loss: 26031.384616\n",
      "Epoch 86, loss: 27118.470799\n",
      "Epoch 87, loss: 26557.776621\n",
      "Epoch 88, loss: 25649.050311\n",
      "Epoch 89, loss: 28264.967182\n",
      "Epoch 90, loss: 26047.917391\n",
      "Epoch 91, loss: 27198.897003\n",
      "Epoch 92, loss: 26508.822110\n",
      "Epoch 93, loss: 26476.710301\n",
      "Epoch 94, loss: 26580.695294\n",
      "Epoch 95, loss: 27316.583802\n",
      "Epoch 96, loss: 27275.455629\n",
      "Epoch 97, loss: 28034.267089\n",
      "Epoch 98, loss: 25086.757943\n",
      "Epoch 99, loss: 27637.477183\n",
      "Accuracy after training for 100 epochs:  0.146\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 25759.592558\n",
      "Epoch 1, loss: 25460.946816\n",
      "Epoch 2, loss: 25151.515980\n",
      "Epoch 3, loss: 25739.526796\n",
      "Epoch 4, loss: 25623.892221\n",
      "Epoch 5, loss: 24492.844342\n",
      "Epoch 6, loss: 24865.140192\n",
      "Epoch 7, loss: 24484.315318\n",
      "Epoch 8, loss: 25725.197654\n",
      "Epoch 9, loss: 23592.384122\n",
      "Epoch 10, loss: 24061.580241\n",
      "Epoch 11, loss: 26129.167136\n",
      "Epoch 12, loss: 23797.954559\n",
      "Epoch 13, loss: 23698.613272\n",
      "Epoch 14, loss: 24952.172215\n",
      "Epoch 15, loss: 24722.113038\n",
      "Epoch 16, loss: 23558.893476\n",
      "Epoch 17, loss: 23934.593422\n",
      "Epoch 18, loss: 24645.247477\n",
      "Epoch 19, loss: 24968.534430\n",
      "Epoch 20, loss: 23108.452481\n",
      "Epoch 21, loss: 23279.041648\n",
      "Epoch 22, loss: 24038.211386\n",
      "Epoch 23, loss: 24041.039260\n",
      "Epoch 24, loss: 24339.125064\n",
      "Epoch 25, loss: 24752.907949\n",
      "Epoch 26, loss: 23670.110499\n",
      "Epoch 27, loss: 24364.829429\n",
      "Epoch 28, loss: 24665.140888\n",
      "Epoch 29, loss: 24334.846817\n",
      "Epoch 30, loss: 24086.028118\n",
      "Epoch 31, loss: 23117.325172\n",
      "Epoch 32, loss: 24218.457961\n",
      "Epoch 33, loss: 23122.599286\n",
      "Epoch 34, loss: 23754.669108\n",
      "Epoch 35, loss: 23861.991387\n",
      "Epoch 36, loss: 24259.116497\n",
      "Epoch 37, loss: 23567.098392\n",
      "Epoch 38, loss: 24497.221617\n",
      "Epoch 39, loss: 24346.689337\n",
      "Epoch 40, loss: 23272.153696\n",
      "Epoch 41, loss: 23828.587491\n",
      "Epoch 42, loss: 23955.513989\n",
      "Epoch 43, loss: 23271.407068\n",
      "Epoch 44, loss: 23797.635817\n",
      "Epoch 45, loss: 23436.620852\n",
      "Epoch 46, loss: 23810.138560\n",
      "Epoch 47, loss: 23562.291542\n",
      "Epoch 48, loss: 24503.162554\n",
      "Epoch 49, loss: 23540.983022\n",
      "Epoch 50, loss: 22671.272810\n",
      "Epoch 51, loss: 23364.521164\n",
      "Epoch 52, loss: 23110.950654\n",
      "Epoch 53, loss: 24268.286158\n",
      "Epoch 54, loss: 23001.078302\n",
      "Epoch 55, loss: 24398.584890\n",
      "Epoch 56, loss: 23377.321802\n",
      "Epoch 57, loss: 23494.349912\n",
      "Epoch 58, loss: 23233.217108\n",
      "Epoch 59, loss: 23455.651736\n",
      "Epoch 60, loss: 22143.896704\n",
      "Epoch 61, loss: 24088.656981\n",
      "Epoch 62, loss: 24013.892059\n",
      "Epoch 63, loss: 22551.754205\n",
      "Epoch 64, loss: 22879.207958\n",
      "Epoch 65, loss: 22946.186854\n",
      "Epoch 66, loss: 22713.096888\n",
      "Epoch 67, loss: 23231.766326\n",
      "Epoch 68, loss: 24241.190504\n",
      "Epoch 69, loss: 22367.563379\n",
      "Epoch 70, loss: 24028.327608\n",
      "Epoch 71, loss: 23833.010013\n",
      "Epoch 72, loss: 22079.088385\n",
      "Epoch 73, loss: 23717.547245\n",
      "Epoch 74, loss: 21858.703605\n",
      "Epoch 75, loss: 24321.827825\n",
      "Epoch 76, loss: 22980.943383\n",
      "Epoch 77, loss: 22340.118759\n",
      "Epoch 78, loss: 23007.702547\n",
      "Epoch 79, loss: 23524.852455\n",
      "Epoch 80, loss: 23123.896538\n",
      "Epoch 81, loss: 23305.432943\n",
      "Epoch 82, loss: 23640.064396\n",
      "Epoch 83, loss: 21942.912461\n",
      "Epoch 84, loss: 24396.548476\n",
      "Epoch 85, loss: 21339.857772\n",
      "Epoch 86, loss: 23402.605814\n",
      "Epoch 87, loss: 23001.191552\n",
      "Epoch 88, loss: 23123.937464\n",
      "Epoch 89, loss: 22121.898865\n",
      "Epoch 90, loss: 22641.604306\n",
      "Epoch 91, loss: 23578.452735\n",
      "Epoch 92, loss: 23956.112565\n",
      "Epoch 93, loss: 21808.513876\n",
      "Epoch 94, loss: 24122.629376\n",
      "Epoch 95, loss: 23642.535436\n",
      "Epoch 96, loss: 24329.656342\n",
      "Epoch 97, loss: 21301.731433\n",
      "Epoch 98, loss: 23541.233019\n",
      "Epoch 99, loss: 23834.352915\n",
      "Epoch 100, loss: 23466.103659\n",
      "Epoch 101, loss: 22299.799648\n",
      "Epoch 102, loss: 22315.644956\n",
      "Epoch 103, loss: 24053.290408\n",
      "Epoch 104, loss: 22997.381151\n",
      "Epoch 105, loss: 22905.629235\n",
      "Epoch 106, loss: 22343.661808\n",
      "Epoch 107, loss: 23201.440135\n",
      "Epoch 108, loss: 23432.036413\n",
      "Epoch 109, loss: 22795.980525\n",
      "Epoch 110, loss: 23124.469152\n",
      "Epoch 111, loss: 23355.832344\n",
      "Epoch 112, loss: 22841.858744\n",
      "Epoch 113, loss: 22656.856440\n",
      "Epoch 114, loss: 23211.526250\n",
      "Epoch 115, loss: 22500.146625\n",
      "Epoch 116, loss: 22937.354279\n",
      "Epoch 117, loss: 22028.497603\n",
      "Epoch 118, loss: 23420.768671\n",
      "Epoch 119, loss: 23370.249112\n",
      "Epoch 120, loss: 23123.202499\n",
      "Epoch 121, loss: 22169.218535\n",
      "Epoch 122, loss: 23707.668087\n",
      "Epoch 123, loss: 22540.942679\n",
      "Epoch 124, loss: 22405.366011\n",
      "Epoch 125, loss: 21911.606952\n",
      "Epoch 126, loss: 22544.663213\n",
      "Epoch 127, loss: 23482.224690\n",
      "Epoch 128, loss: 23437.930466\n",
      "Epoch 129, loss: 22025.900175\n",
      "Epoch 130, loss: 23550.301068\n",
      "Epoch 131, loss: 22060.396108\n",
      "Epoch 132, loss: 24278.453970\n",
      "Epoch 133, loss: 22617.973107\n",
      "Epoch 134, loss: 21583.909824\n",
      "Epoch 135, loss: 23760.073841\n",
      "Epoch 136, loss: 22763.038346\n",
      "Epoch 137, loss: 22069.529959\n",
      "Epoch 138, loss: 21354.265327\n",
      "Epoch 139, loss: 22054.652567\n",
      "Epoch 140, loss: 23636.077208\n",
      "Epoch 141, loss: 21853.186769\n",
      "Epoch 142, loss: 23458.554656\n",
      "Epoch 143, loss: 21486.741057\n",
      "Epoch 144, loss: 23070.580434\n",
      "Epoch 145, loss: 22409.467256\n",
      "Epoch 146, loss: 22764.899174\n",
      "Epoch 147, loss: 24791.962112\n",
      "Epoch 148, loss: 22284.489968\n",
      "Epoch 149, loss: 21776.292272\n",
      "Epoch 150, loss: 23092.247357\n",
      "Epoch 151, loss: 22768.455788\n",
      "Epoch 152, loss: 22129.787513\n",
      "Epoch 153, loss: 23101.559776\n",
      "Epoch 154, loss: 21851.093840\n",
      "Epoch 155, loss: 23112.829595\n",
      "Epoch 156, loss: 21381.819471\n",
      "Epoch 157, loss: 22281.898798\n",
      "Epoch 158, loss: 23327.937888\n",
      "Epoch 159, loss: 21797.059882\n",
      "Epoch 160, loss: 21842.139616\n",
      "Epoch 161, loss: 23321.081506\n",
      "Epoch 162, loss: 22588.031108\n",
      "Epoch 163, loss: 21143.491107\n",
      "Epoch 164, loss: 22242.215623\n",
      "Epoch 165, loss: 22962.802226\n",
      "Epoch 166, loss: 22146.747096\n",
      "Epoch 167, loss: 22514.365721\n",
      "Epoch 168, loss: 22415.220018\n",
      "Epoch 169, loss: 22999.823218\n",
      "Epoch 170, loss: 22044.264039\n",
      "Epoch 171, loss: 22862.103299\n",
      "Epoch 172, loss: 21687.829776\n",
      "Epoch 173, loss: 23614.255874\n",
      "Epoch 174, loss: 22403.409889\n",
      "Epoch 175, loss: 22552.707249\n",
      "Epoch 176, loss: 21300.644804\n",
      "Epoch 177, loss: 22370.968950\n",
      "Epoch 178, loss: 22220.708638\n",
      "Epoch 179, loss: 22872.339504\n",
      "Epoch 180, loss: 23202.427666\n",
      "Epoch 181, loss: 22151.146191\n",
      "Epoch 182, loss: 22079.776481\n",
      "Epoch 183, loss: 21487.969238\n",
      "Epoch 184, loss: 21995.956444\n",
      "Epoch 185, loss: 21522.624776\n",
      "Epoch 186, loss: 23290.209591\n",
      "Epoch 187, loss: 21269.468371\n",
      "Epoch 188, loss: 23703.646954\n",
      "Epoch 189, loss: 21828.608042\n",
      "Epoch 190, loss: 21308.144465\n",
      "Epoch 191, loss: 21488.623456\n",
      "Epoch 192, loss: 22480.658823\n",
      "Epoch 193, loss: 23048.621629\n",
      "Epoch 194, loss: 22570.503304\n",
      "Epoch 195, loss: 22726.414747\n",
      "Epoch 196, loss: 22439.319642\n",
      "Epoch 197, loss: 21678.409415\n",
      "Epoch 198, loss: 21435.661214\n",
      "Epoch 199, loss: 22661.224430\n",
      "0.001 0.0001 0.187\n",
      "Epoch 0, loss: 25461.515065\n",
      "Epoch 1, loss: 26584.857078\n",
      "Epoch 2, loss: 25826.549321\n",
      "Epoch 3, loss: 26218.576884\n",
      "Epoch 4, loss: 26595.993973\n",
      "Epoch 5, loss: 23740.460647\n",
      "Epoch 6, loss: 25240.034029\n",
      "Epoch 7, loss: 24229.306433\n",
      "Epoch 8, loss: 25126.515093\n",
      "Epoch 9, loss: 25721.594504\n",
      "Epoch 10, loss: 24530.571213\n",
      "Epoch 11, loss: 23922.127050\n",
      "Epoch 12, loss: 24913.597792\n",
      "Epoch 13, loss: 23888.322290\n",
      "Epoch 14, loss: 24671.915608\n",
      "Epoch 15, loss: 24309.222811\n",
      "Epoch 16, loss: 25001.785951\n",
      "Epoch 17, loss: 24241.513146\n",
      "Epoch 18, loss: 25653.162434\n",
      "Epoch 19, loss: 24148.001181\n",
      "Epoch 20, loss: 23946.475316\n",
      "Epoch 21, loss: 25126.706720\n",
      "Epoch 22, loss: 23765.146125\n",
      "Epoch 23, loss: 24198.145039\n",
      "Epoch 24, loss: 23423.971208\n",
      "Epoch 25, loss: 24279.631251\n",
      "Epoch 26, loss: 23006.084066\n",
      "Epoch 27, loss: 23492.871335\n",
      "Epoch 28, loss: 25946.961738\n",
      "Epoch 29, loss: 24128.470096\n",
      "Epoch 30, loss: 25127.840078\n",
      "Epoch 31, loss: 23703.617885\n",
      "Epoch 32, loss: 24447.348960\n",
      "Epoch 33, loss: 23473.013222\n",
      "Epoch 34, loss: 23600.101083\n",
      "Epoch 35, loss: 23336.791773\n",
      "Epoch 36, loss: 22777.742999\n",
      "Epoch 37, loss: 21719.069073\n",
      "Epoch 38, loss: 24877.467489\n",
      "Epoch 39, loss: 23781.120273\n",
      "Epoch 40, loss: 24499.745388\n",
      "Epoch 41, loss: 23994.115301\n",
      "Epoch 42, loss: 22877.241138\n",
      "Epoch 43, loss: 24283.858727\n",
      "Epoch 44, loss: 24237.860589\n",
      "Epoch 45, loss: 23547.760543\n",
      "Epoch 46, loss: 23843.522976\n",
      "Epoch 47, loss: 24469.892589\n",
      "Epoch 48, loss: 22357.518648\n",
      "Epoch 49, loss: 23526.448330\n",
      "Epoch 50, loss: 23289.235787\n",
      "Epoch 51, loss: 22819.601053\n",
      "Epoch 52, loss: 24226.141508\n",
      "Epoch 53, loss: 24497.421410\n",
      "Epoch 54, loss: 22477.979853\n",
      "Epoch 55, loss: 24005.851236\n",
      "Epoch 56, loss: 23877.483230\n",
      "Epoch 57, loss: 24488.672797\n",
      "Epoch 58, loss: 23627.656995\n",
      "Epoch 59, loss: 23175.784901\n",
      "Epoch 60, loss: 22672.670254\n",
      "Epoch 61, loss: 23166.092032\n",
      "Epoch 62, loss: 24104.808934\n",
      "Epoch 63, loss: 23183.398619\n",
      "Epoch 64, loss: 24488.389594\n",
      "Epoch 65, loss: 22156.912677\n",
      "Epoch 66, loss: 22321.758458\n",
      "Epoch 67, loss: 24695.714181\n",
      "Epoch 68, loss: 22149.353511\n",
      "Epoch 69, loss: 23969.245919\n",
      "Epoch 70, loss: 23228.615730\n",
      "Epoch 71, loss: 23618.181190\n",
      "Epoch 72, loss: 21660.888493\n",
      "Epoch 73, loss: 22707.503578\n",
      "Epoch 74, loss: 24288.464568\n",
      "Epoch 75, loss: 23279.296711\n",
      "Epoch 76, loss: 23648.869608\n",
      "Epoch 77, loss: 22888.956172\n",
      "Epoch 78, loss: 22839.554022\n",
      "Epoch 79, loss: 23335.634799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, loss: 22553.929906\n",
      "Epoch 81, loss: 23053.876223\n",
      "Epoch 82, loss: 23118.220127\n",
      "Epoch 83, loss: 22655.573412\n",
      "Epoch 84, loss: 24240.324176\n",
      "Epoch 85, loss: 23646.835150\n",
      "Epoch 86, loss: 21541.942275\n",
      "Epoch 87, loss: 24746.440206\n",
      "Epoch 88, loss: 22981.770711\n",
      "Epoch 89, loss: 21651.638458\n",
      "Epoch 90, loss: 23445.242805\n",
      "Epoch 91, loss: 21981.157493\n",
      "Epoch 92, loss: 24679.581035\n",
      "Epoch 93, loss: 22829.122291\n",
      "Epoch 94, loss: 22986.733934\n",
      "Epoch 95, loss: 24407.743759\n",
      "Epoch 96, loss: 22141.831393\n",
      "Epoch 97, loss: 22153.829933\n",
      "Epoch 98, loss: 24163.670127\n",
      "Epoch 99, loss: 22899.109047\n",
      "Epoch 100, loss: 23814.504837\n",
      "Epoch 101, loss: 22931.513681\n",
      "Epoch 102, loss: 21371.880859\n",
      "Epoch 103, loss: 22660.708277\n",
      "Epoch 104, loss: 22160.489656\n",
      "Epoch 105, loss: 22466.172849\n",
      "Epoch 106, loss: 22548.639147\n",
      "Epoch 107, loss: 22656.009650\n",
      "Epoch 108, loss: 22870.969768\n",
      "Epoch 109, loss: 22685.621847\n",
      "Epoch 110, loss: 22887.902765\n",
      "Epoch 111, loss: 23952.995662\n",
      "Epoch 112, loss: 21817.728167\n",
      "Epoch 113, loss: 23341.750652\n",
      "Epoch 114, loss: 20968.023149\n",
      "Epoch 115, loss: 23501.442216\n",
      "Epoch 116, loss: 23283.216729\n",
      "Epoch 117, loss: 23013.781297\n",
      "Epoch 118, loss: 22518.269996\n",
      "Epoch 119, loss: 22975.876856\n",
      "Epoch 120, loss: 22615.807517\n",
      "Epoch 121, loss: 23320.017203\n",
      "Epoch 122, loss: 22312.463536\n",
      "Epoch 123, loss: 22570.765239\n",
      "Epoch 124, loss: 21929.797995\n",
      "Epoch 125, loss: 22323.057487\n",
      "Epoch 126, loss: 21758.764337\n",
      "Epoch 127, loss: 22657.668840\n",
      "Epoch 128, loss: 23142.054814\n",
      "Epoch 129, loss: 22121.720060\n",
      "Epoch 130, loss: 21214.096378\n",
      "Epoch 131, loss: 23228.802623\n",
      "Epoch 132, loss: 23085.848671\n",
      "Epoch 133, loss: 22680.331451\n",
      "Epoch 134, loss: 22854.169567\n",
      "Epoch 135, loss: 21690.114644\n",
      "Epoch 136, loss: 23660.812821\n",
      "Epoch 137, loss: 21536.028773\n",
      "Epoch 138, loss: 23865.244259\n",
      "Epoch 139, loss: 24205.008902\n",
      "Epoch 140, loss: 21898.406805\n",
      "Epoch 141, loss: 22781.603775\n",
      "Epoch 142, loss: 21241.772731\n",
      "Epoch 143, loss: 23527.627185\n",
      "Epoch 144, loss: 22701.329120\n",
      "Epoch 145, loss: 22154.966519\n",
      "Epoch 146, loss: 22990.552005\n",
      "Epoch 147, loss: 20538.487182\n",
      "Epoch 148, loss: 23100.584235\n",
      "Epoch 149, loss: 21894.470775\n",
      "Epoch 150, loss: 21738.783417\n",
      "Epoch 151, loss: 22976.749975\n",
      "Epoch 152, loss: 23575.538101\n",
      "Epoch 153, loss: 22705.303313\n",
      "Epoch 154, loss: 21333.281035\n",
      "Epoch 155, loss: 23297.019397\n",
      "Epoch 156, loss: 22947.755111\n",
      "Epoch 157, loss: 22267.225919\n",
      "Epoch 158, loss: 22421.216490\n",
      "Epoch 159, loss: 23339.496404\n",
      "Epoch 160, loss: 21525.516420\n",
      "Epoch 161, loss: 22378.909737\n",
      "Epoch 162, loss: 23007.449873\n",
      "Epoch 163, loss: 22071.720441\n",
      "Epoch 164, loss: 22564.914865\n",
      "Epoch 165, loss: 23510.835692\n",
      "Epoch 166, loss: 22048.265537\n",
      "Epoch 167, loss: 23440.550807\n",
      "Epoch 168, loss: 21473.698120\n",
      "Epoch 169, loss: 21904.128626\n",
      "Epoch 170, loss: 23252.363345\n",
      "Epoch 171, loss: 21215.090794\n",
      "Epoch 172, loss: 21576.231678\n",
      "Epoch 173, loss: 23576.317208\n",
      "Epoch 174, loss: 22940.361294\n",
      "Epoch 175, loss: 22173.505695\n",
      "Epoch 176, loss: 23608.237513\n",
      "Epoch 177, loss: 22136.154941\n",
      "Epoch 178, loss: 21849.995285\n",
      "Epoch 179, loss: 21786.853604\n",
      "Epoch 180, loss: 21500.600895\n",
      "Epoch 181, loss: 22671.720806\n",
      "Epoch 182, loss: 20833.004732\n",
      "Epoch 183, loss: 21091.679041\n",
      "Epoch 184, loss: 21953.126297\n",
      "Epoch 185, loss: 23500.642180\n",
      "Epoch 186, loss: 21042.602933\n",
      "Epoch 187, loss: 22833.959839\n",
      "Epoch 188, loss: 22464.550368\n",
      "Epoch 189, loss: 22002.609233\n",
      "Epoch 190, loss: 19832.150749\n",
      "Epoch 191, loss: 23050.270018\n",
      "Epoch 192, loss: 22585.207658\n",
      "Epoch 193, loss: 22943.659783\n",
      "Epoch 194, loss: 22299.162902\n",
      "Epoch 195, loss: 22693.801413\n",
      "Epoch 196, loss: 22085.783579\n",
      "Epoch 197, loss: 22601.424414\n",
      "Epoch 198, loss: 22344.172645\n",
      "Epoch 199, loss: 21758.124997\n",
      "0.001 1e-05 0.196\n",
      "Epoch 0, loss: 25980.196442\n",
      "Epoch 1, loss: 25349.962750\n",
      "Epoch 2, loss: 24862.114238\n",
      "Epoch 3, loss: 25986.330987\n",
      "Epoch 4, loss: 24776.285261\n",
      "Epoch 5, loss: 26357.458844\n",
      "Epoch 6, loss: 25935.489241\n",
      "Epoch 7, loss: 24975.519630\n",
      "Epoch 8, loss: 24653.281656\n",
      "Epoch 9, loss: 24770.224746\n",
      "Epoch 10, loss: 25559.717162\n",
      "Epoch 11, loss: 23562.908949\n",
      "Epoch 12, loss: 24865.837187\n",
      "Epoch 13, loss: 24572.421174\n",
      "Epoch 14, loss: 23655.060675\n",
      "Epoch 15, loss: 24276.756977\n",
      "Epoch 16, loss: 22492.720070\n",
      "Epoch 17, loss: 26080.478474\n",
      "Epoch 18, loss: 23652.761179\n",
      "Epoch 19, loss: 22313.253986\n",
      "Epoch 20, loss: 25725.107396\n",
      "Epoch 21, loss: 24246.929446\n",
      "Epoch 22, loss: 23739.808192\n",
      "Epoch 23, loss: 26130.308653\n",
      "Epoch 24, loss: 24064.273214\n",
      "Epoch 25, loss: 24160.944649\n",
      "Epoch 26, loss: 26003.591575\n",
      "Epoch 27, loss: 23844.326344\n",
      "Epoch 28, loss: 24408.718508\n",
      "Epoch 29, loss: 22857.205601\n",
      "Epoch 30, loss: 24359.741826\n",
      "Epoch 31, loss: 23214.013581\n",
      "Epoch 32, loss: 23453.335452\n",
      "Epoch 33, loss: 24866.388983\n",
      "Epoch 34, loss: 23345.765077\n",
      "Epoch 35, loss: 23629.371093\n",
      "Epoch 36, loss: 24474.082594\n",
      "Epoch 37, loss: 24168.006963\n",
      "Epoch 38, loss: 23321.295272\n",
      "Epoch 39, loss: 24566.204071\n",
      "Epoch 40, loss: 23488.408618\n",
      "Epoch 41, loss: 23129.834560\n",
      "Epoch 42, loss: 23685.159495\n",
      "Epoch 43, loss: 25232.505525\n",
      "Epoch 44, loss: 23270.905433\n",
      "Epoch 45, loss: 24941.202401\n",
      "Epoch 46, loss: 22536.517929\n",
      "Epoch 47, loss: 24621.809851\n",
      "Epoch 48, loss: 24493.664367\n",
      "Epoch 49, loss: 21664.655499\n",
      "Epoch 50, loss: 23902.591058\n",
      "Epoch 51, loss: 23192.183956\n",
      "Epoch 52, loss: 24147.631551\n",
      "Epoch 53, loss: 24093.295209\n",
      "Epoch 54, loss: 22565.250223\n",
      "Epoch 55, loss: 23841.286272\n",
      "Epoch 56, loss: 24299.696340\n",
      "Epoch 57, loss: 24092.708568\n",
      "Epoch 58, loss: 24635.954945\n",
      "Epoch 59, loss: 24638.380960\n",
      "Epoch 60, loss: 22212.127811\n",
      "Epoch 61, loss: 23370.949868\n",
      "Epoch 62, loss: 22855.527267\n",
      "Epoch 63, loss: 23466.001042\n",
      "Epoch 64, loss: 23869.164650\n",
      "Epoch 65, loss: 23605.325323\n",
      "Epoch 66, loss: 23176.133414\n",
      "Epoch 67, loss: 23133.080317\n",
      "Epoch 68, loss: 23142.131849\n",
      "Epoch 69, loss: 24466.959228\n",
      "Epoch 70, loss: 22984.177499\n",
      "Epoch 71, loss: 21759.959073\n",
      "Epoch 72, loss: 23406.132005\n",
      "Epoch 73, loss: 22634.818865\n",
      "Epoch 74, loss: 23566.340095\n",
      "Epoch 75, loss: 23075.355200\n",
      "Epoch 76, loss: 24020.861938\n",
      "Epoch 77, loss: 23846.327329\n",
      "Epoch 78, loss: 22169.537929\n",
      "Epoch 79, loss: 22881.721363\n",
      "Epoch 80, loss: 23432.059293\n",
      "Epoch 81, loss: 22753.644985\n",
      "Epoch 82, loss: 23909.272862\n",
      "Epoch 83, loss: 23612.407351\n",
      "Epoch 84, loss: 22632.244910\n",
      "Epoch 85, loss: 21769.399108\n",
      "Epoch 86, loss: 22243.847281\n",
      "Epoch 87, loss: 22650.939714\n",
      "Epoch 88, loss: 24150.328288\n",
      "Epoch 89, loss: 23195.869491\n",
      "Epoch 90, loss: 23223.219125\n",
      "Epoch 91, loss: 22825.697348\n",
      "Epoch 92, loss: 22245.331673\n",
      "Epoch 93, loss: 21442.028488\n",
      "Epoch 94, loss: 24077.239701\n",
      "Epoch 95, loss: 22484.370733\n",
      "Epoch 96, loss: 23343.315845\n",
      "Epoch 97, loss: 22841.496046\n",
      "Epoch 98, loss: 23527.650980\n",
      "Epoch 99, loss: 22137.319497\n",
      "Epoch 100, loss: 22538.733201\n",
      "Epoch 101, loss: 23488.058278\n",
      "Epoch 102, loss: 23272.925257\n",
      "Epoch 103, loss: 23239.607183\n",
      "Epoch 104, loss: 22951.047906\n",
      "Epoch 105, loss: 23351.589444\n",
      "Epoch 106, loss: 22518.493566\n",
      "Epoch 107, loss: 22896.651006\n",
      "Epoch 108, loss: 22211.850344\n",
      "Epoch 109, loss: 22929.092880\n",
      "Epoch 110, loss: 21931.736971\n",
      "Epoch 111, loss: 22644.495955\n",
      "Epoch 112, loss: 21652.116845\n",
      "Epoch 113, loss: 22514.336023\n",
      "Epoch 114, loss: 23640.353329\n",
      "Epoch 115, loss: 23117.907112\n",
      "Epoch 116, loss: 22899.537558\n",
      "Epoch 117, loss: 23641.511026\n",
      "Epoch 118, loss: 22139.421683\n",
      "Epoch 119, loss: 23578.864282\n",
      "Epoch 120, loss: 22292.169422\n",
      "Epoch 121, loss: 22022.525409\n",
      "Epoch 122, loss: 21803.302970\n",
      "Epoch 123, loss: 22456.116770\n",
      "Epoch 124, loss: 24040.278969\n",
      "Epoch 125, loss: 22392.089149\n",
      "Epoch 126, loss: 21931.381627\n",
      "Epoch 127, loss: 21757.686337\n",
      "Epoch 128, loss: 21408.418655\n",
      "Epoch 129, loss: 24434.711617\n",
      "Epoch 130, loss: 22515.352872\n",
      "Epoch 131, loss: 23050.254042\n",
      "Epoch 132, loss: 22339.898299\n",
      "Epoch 133, loss: 22579.545139\n",
      "Epoch 134, loss: 22353.173491\n",
      "Epoch 135, loss: 22529.727844\n",
      "Epoch 136, loss: 23647.126734\n",
      "Epoch 137, loss: 21457.104586\n",
      "Epoch 138, loss: 22713.422711\n",
      "Epoch 139, loss: 22963.622843\n",
      "Epoch 140, loss: 22276.956738\n",
      "Epoch 141, loss: 23141.271627\n",
      "Epoch 142, loss: 22073.647226\n",
      "Epoch 143, loss: 23230.974705\n",
      "Epoch 144, loss: 21763.977307\n",
      "Epoch 145, loss: 22365.167820\n",
      "Epoch 146, loss: 22482.778878\n",
      "Epoch 147, loss: 21727.942156\n",
      "Epoch 148, loss: 22312.155900\n",
      "Epoch 149, loss: 22129.388772\n",
      "Epoch 150, loss: 22991.242744\n",
      "Epoch 151, loss: 23309.681608\n",
      "Epoch 152, loss: 22230.200308\n",
      "Epoch 153, loss: 21849.833422\n",
      "Epoch 154, loss: 22112.562833\n",
      "Epoch 155, loss: 20778.943431\n",
      "Epoch 156, loss: 22782.028148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157, loss: 21945.337259\n",
      "Epoch 158, loss: 23000.055812\n",
      "Epoch 159, loss: 22335.121256\n",
      "Epoch 160, loss: 23367.964970\n",
      "Epoch 161, loss: 22051.603263\n",
      "Epoch 162, loss: 22977.209961\n",
      "Epoch 163, loss: 21824.754300\n",
      "Epoch 164, loss: 20727.071544\n",
      "Epoch 165, loss: 22075.235742\n",
      "Epoch 166, loss: 21676.239865\n",
      "Epoch 167, loss: 21257.049760\n",
      "Epoch 168, loss: 22192.980350\n",
      "Epoch 169, loss: 22384.451043\n",
      "Epoch 170, loss: 22173.312504\n",
      "Epoch 171, loss: 22773.087797\n",
      "Epoch 172, loss: 22874.817054\n",
      "Epoch 173, loss: 23156.998185\n",
      "Epoch 174, loss: 21461.142053\n",
      "Epoch 175, loss: 22847.873619\n",
      "Epoch 176, loss: 21754.804559\n",
      "Epoch 177, loss: 22045.983014\n",
      "Epoch 178, loss: 22411.944908\n",
      "Epoch 179, loss: 22578.273555\n",
      "Epoch 180, loss: 23941.159999\n",
      "Epoch 181, loss: 21714.617845\n",
      "Epoch 182, loss: 21150.939850\n",
      "Epoch 183, loss: 22994.811719\n",
      "Epoch 184, loss: 21787.483093\n",
      "Epoch 185, loss: 22606.581875\n",
      "Epoch 186, loss: 21976.357875\n",
      "Epoch 187, loss: 22484.615748\n",
      "Epoch 188, loss: 23000.120571\n",
      "Epoch 189, loss: 22092.791229\n",
      "Epoch 190, loss: 22152.696912\n",
      "Epoch 191, loss: 23307.982964\n",
      "Epoch 192, loss: 22022.665013\n",
      "Epoch 193, loss: 21286.978047\n",
      "Epoch 194, loss: 22380.501694\n",
      "Epoch 195, loss: 22341.938606\n",
      "Epoch 196, loss: 22269.746836\n",
      "Epoch 197, loss: 22989.519293\n",
      "Epoch 198, loss: 21299.791808\n",
      "Epoch 199, loss: 24312.850983\n",
      "0.001 1e-06 0.203\n",
      "Epoch 0, loss: 20645.004856\n",
      "Epoch 1, loss: 20450.317094\n",
      "Epoch 2, loss: 20295.852344\n",
      "Epoch 3, loss: 20155.793417\n",
      "Epoch 4, loss: 20048.208574\n",
      "Epoch 5, loss: 19950.996634\n",
      "Epoch 6, loss: 19861.099088\n",
      "Epoch 7, loss: 19794.952912\n",
      "Epoch 8, loss: 19740.489225\n",
      "Epoch 9, loss: 19679.471883\n",
      "Epoch 10, loss: 19638.835217\n",
      "Epoch 11, loss: 19594.457752\n",
      "Epoch 12, loss: 19558.079324\n",
      "Epoch 13, loss: 19526.520838\n",
      "Epoch 14, loss: 19497.435868\n",
      "Epoch 15, loss: 19468.440180\n",
      "Epoch 16, loss: 19443.446215\n",
      "Epoch 17, loss: 19421.259732\n",
      "Epoch 18, loss: 19393.851926\n",
      "Epoch 19, loss: 19369.977890\n",
      "Epoch 20, loss: 19356.615458\n",
      "Epoch 21, loss: 19334.853026\n",
      "Epoch 22, loss: 19318.543697\n",
      "Epoch 23, loss: 19301.870414\n",
      "Epoch 24, loss: 19289.883759\n",
      "Epoch 25, loss: 19271.646443\n",
      "Epoch 26, loss: 19250.770903\n",
      "Epoch 27, loss: 19244.195725\n",
      "Epoch 28, loss: 19231.975842\n",
      "Epoch 29, loss: 19218.941935\n",
      "Epoch 30, loss: 19198.948621\n",
      "Epoch 31, loss: 19192.770445\n",
      "Epoch 32, loss: 19178.843515\n",
      "Epoch 33, loss: 19170.701557\n",
      "Epoch 34, loss: 19159.909155\n",
      "Epoch 35, loss: 19141.869529\n",
      "Epoch 36, loss: 19138.239068\n",
      "Epoch 37, loss: 19137.040521\n",
      "Epoch 38, loss: 19121.955119\n",
      "Epoch 39, loss: 19113.488510\n",
      "Epoch 40, loss: 19098.802788\n",
      "Epoch 41, loss: 19094.863130\n",
      "Epoch 42, loss: 19088.223942\n",
      "Epoch 43, loss: 19077.050168\n",
      "Epoch 44, loss: 19060.030360\n",
      "Epoch 45, loss: 19060.637393\n",
      "Epoch 46, loss: 19052.958062\n",
      "Epoch 47, loss: 19042.003542\n",
      "Epoch 48, loss: 19039.569024\n",
      "Epoch 49, loss: 19031.918527\n",
      "Epoch 50, loss: 19022.136880\n",
      "Epoch 51, loss: 19011.525061\n",
      "Epoch 52, loss: 19012.877182\n",
      "Epoch 53, loss: 19007.481618\n",
      "Epoch 54, loss: 18994.099396\n",
      "Epoch 55, loss: 18991.404502\n",
      "Epoch 56, loss: 18986.550207\n",
      "Epoch 57, loss: 18981.619975\n",
      "Epoch 58, loss: 18971.310543\n",
      "Epoch 59, loss: 18966.792756\n",
      "Epoch 60, loss: 18963.158226\n",
      "Epoch 61, loss: 18951.121580\n",
      "Epoch 62, loss: 18949.406105\n",
      "Epoch 63, loss: 18934.703540\n",
      "Epoch 64, loss: 18933.517572\n",
      "Epoch 65, loss: 18928.040687\n",
      "Epoch 66, loss: 18924.572120\n",
      "Epoch 67, loss: 18919.122881\n",
      "Epoch 68, loss: 18910.974690\n",
      "Epoch 69, loss: 18912.610761\n",
      "Epoch 70, loss: 18905.433501\n",
      "Epoch 71, loss: 18895.008354\n",
      "Epoch 72, loss: 18894.257975\n",
      "Epoch 73, loss: 18884.722705\n",
      "Epoch 74, loss: 18882.506871\n",
      "Epoch 75, loss: 18880.015161\n",
      "Epoch 76, loss: 18876.137784\n",
      "Epoch 77, loss: 18870.919727\n",
      "Epoch 78, loss: 18869.658321\n",
      "Epoch 79, loss: 18860.224122\n",
      "Epoch 80, loss: 18849.597443\n",
      "Epoch 81, loss: 18849.718811\n",
      "Epoch 82, loss: 18846.921496\n",
      "Epoch 83, loss: 18836.747043\n",
      "Epoch 84, loss: 18841.702183\n",
      "Epoch 85, loss: 18834.206809\n",
      "Epoch 86, loss: 18830.436870\n",
      "Epoch 87, loss: 18824.387204\n",
      "Epoch 88, loss: 18824.192837\n",
      "Epoch 89, loss: 18819.706481\n",
      "Epoch 90, loss: 18813.910260\n",
      "Epoch 91, loss: 18810.881891\n",
      "Epoch 92, loss: 18808.134421\n",
      "Epoch 93, loss: 18802.951137\n",
      "Epoch 94, loss: 18792.143728\n",
      "Epoch 95, loss: 18799.930871\n",
      "Epoch 96, loss: 18789.936370\n",
      "Epoch 97, loss: 18785.723500\n",
      "Epoch 98, loss: 18784.880140\n",
      "Epoch 99, loss: 18778.589214\n",
      "Epoch 100, loss: 18779.179356\n",
      "Epoch 101, loss: 18774.191526\n",
      "Epoch 102, loss: 18764.435091\n",
      "Epoch 103, loss: 18771.086158\n",
      "Epoch 104, loss: 18761.063789\n",
      "Epoch 105, loss: 18754.804688\n",
      "Epoch 106, loss: 18756.300043\n",
      "Epoch 107, loss: 18753.082354\n",
      "Epoch 108, loss: 18745.319394\n",
      "Epoch 109, loss: 18745.652825\n",
      "Epoch 110, loss: 18741.058597\n",
      "Epoch 111, loss: 18741.501742\n",
      "Epoch 112, loss: 18734.174843\n",
      "Epoch 113, loss: 18734.247879\n",
      "Epoch 114, loss: 18733.073501\n",
      "Epoch 115, loss: 18726.873046\n",
      "Epoch 116, loss: 18724.300099\n",
      "Epoch 117, loss: 18724.248478\n",
      "Epoch 118, loss: 18712.793150\n",
      "Epoch 119, loss: 18706.345343\n",
      "Epoch 120, loss: 18709.101805\n",
      "Epoch 121, loss: 18709.344050\n",
      "Epoch 122, loss: 18703.500065\n",
      "Epoch 123, loss: 18701.143961\n",
      "Epoch 124, loss: 18695.334493\n",
      "Epoch 125, loss: 18700.232866\n",
      "Epoch 126, loss: 18689.543553\n",
      "Epoch 127, loss: 18687.898734\n",
      "Epoch 128, loss: 18686.456053\n",
      "Epoch 129, loss: 18683.574314\n",
      "Epoch 130, loss: 18680.249600\n",
      "Epoch 131, loss: 18677.569247\n",
      "Epoch 132, loss: 18677.348047\n",
      "Epoch 133, loss: 18676.893717\n",
      "Epoch 134, loss: 18675.003413\n",
      "Epoch 135, loss: 18665.985404\n",
      "Epoch 136, loss: 18662.808389\n",
      "Epoch 137, loss: 18660.659824\n",
      "Epoch 138, loss: 18658.665401\n",
      "Epoch 139, loss: 18656.800062\n",
      "Epoch 140, loss: 18650.687957\n",
      "Epoch 141, loss: 18650.040120\n",
      "Epoch 142, loss: 18647.086232\n",
      "Epoch 143, loss: 18643.283831\n",
      "Epoch 144, loss: 18645.543735\n",
      "Epoch 145, loss: 18642.114413\n",
      "Epoch 146, loss: 18637.248162\n",
      "Epoch 147, loss: 18633.643516\n",
      "Epoch 148, loss: 18632.437505\n",
      "Epoch 149, loss: 18631.849348\n",
      "Epoch 150, loss: 18630.458756\n",
      "Epoch 151, loss: 18624.360957\n",
      "Epoch 152, loss: 18628.075667\n",
      "Epoch 153, loss: 18622.450967\n",
      "Epoch 154, loss: 18618.782205\n",
      "Epoch 155, loss: 18608.730862\n",
      "Epoch 156, loss: 18619.690752\n",
      "Epoch 157, loss: 18610.542806\n",
      "Epoch 158, loss: 18611.910274\n",
      "Epoch 159, loss: 18612.386125\n",
      "Epoch 160, loss: 18604.911693\n",
      "Epoch 161, loss: 18597.896603\n",
      "Epoch 162, loss: 18598.367837\n",
      "Epoch 163, loss: 18590.995726\n",
      "Epoch 164, loss: 18594.815954\n",
      "Epoch 165, loss: 18587.862318\n",
      "Epoch 166, loss: 18596.920242\n",
      "Epoch 167, loss: 18587.882019\n",
      "Epoch 168, loss: 18590.331163\n",
      "Epoch 169, loss: 18581.107040\n",
      "Epoch 170, loss: 18579.576585\n",
      "Epoch 171, loss: 18579.927150\n",
      "Epoch 172, loss: 18578.672491\n",
      "Epoch 173, loss: 18577.969478\n",
      "Epoch 174, loss: 18572.936555\n",
      "Epoch 175, loss: 18566.369696\n",
      "Epoch 176, loss: 18568.298668\n",
      "Epoch 177, loss: 18565.955884\n",
      "Epoch 178, loss: 18563.284628\n",
      "Epoch 179, loss: 18563.253586\n",
      "Epoch 180, loss: 18562.093866\n",
      "Epoch 181, loss: 18556.496199\n",
      "Epoch 182, loss: 18549.647798\n",
      "Epoch 183, loss: 18557.996691\n",
      "Epoch 184, loss: 18548.103121\n",
      "Epoch 185, loss: 18542.434900\n",
      "Epoch 186, loss: 18548.225877\n",
      "Epoch 187, loss: 18544.369600\n",
      "Epoch 188, loss: 18538.211982\n",
      "Epoch 189, loss: 18538.451417\n",
      "Epoch 190, loss: 18538.655163\n",
      "Epoch 191, loss: 18531.684100\n",
      "Epoch 192, loss: 18533.230486\n",
      "Epoch 193, loss: 18527.070664\n",
      "Epoch 194, loss: 18529.760051\n",
      "Epoch 195, loss: 18524.513870\n",
      "Epoch 196, loss: 18526.536026\n",
      "Epoch 197, loss: 18518.146085\n",
      "Epoch 198, loss: 18518.276596\n",
      "Epoch 199, loss: 18518.336962\n",
      "0.0001 0.0001 0.247\n",
      "Epoch 0, loss: 20650.416608\n",
      "Epoch 1, loss: 20448.145943\n",
      "Epoch 2, loss: 20287.750035\n",
      "Epoch 3, loss: 20151.277240\n",
      "Epoch 4, loss: 20045.144571\n",
      "Epoch 5, loss: 19953.352107\n",
      "Epoch 6, loss: 19864.835106\n",
      "Epoch 7, loss: 19796.798439\n",
      "Epoch 8, loss: 19736.513695\n",
      "Epoch 9, loss: 19681.322967\n",
      "Epoch 10, loss: 19636.364201\n",
      "Epoch 11, loss: 19591.771327\n",
      "Epoch 12, loss: 19556.301053\n",
      "Epoch 13, loss: 19523.377181\n",
      "Epoch 14, loss: 19490.129548\n",
      "Epoch 15, loss: 19469.686238\n",
      "Epoch 16, loss: 19440.618697\n",
      "Epoch 17, loss: 19414.885759\n",
      "Epoch 18, loss: 19398.508006\n",
      "Epoch 19, loss: 19371.013108\n",
      "Epoch 20, loss: 19351.989838\n",
      "Epoch 21, loss: 19338.058270\n",
      "Epoch 22, loss: 19314.783373\n",
      "Epoch 23, loss: 19300.193995\n",
      "Epoch 24, loss: 19286.862990\n",
      "Epoch 25, loss: 19270.526907\n",
      "Epoch 26, loss: 19255.918611\n",
      "Epoch 27, loss: 19240.856368\n",
      "Epoch 28, loss: 19225.050725\n",
      "Epoch 29, loss: 19219.272408\n",
      "Epoch 30, loss: 19200.177515\n",
      "Epoch 31, loss: 19193.318050\n",
      "Epoch 32, loss: 19178.765094\n",
      "Epoch 33, loss: 19170.898952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, loss: 19157.832216\n",
      "Epoch 35, loss: 19145.973308\n",
      "Epoch 36, loss: 19137.342690\n",
      "Epoch 37, loss: 19131.520695\n",
      "Epoch 38, loss: 19118.560758\n",
      "Epoch 39, loss: 19104.337862\n",
      "Epoch 40, loss: 19101.840275\n",
      "Epoch 41, loss: 19089.283360\n",
      "Epoch 42, loss: 19081.642290\n",
      "Epoch 43, loss: 19079.270659\n",
      "Epoch 44, loss: 19060.913334\n",
      "Epoch 45, loss: 19047.501851\n",
      "Epoch 46, loss: 19053.614548\n",
      "Epoch 47, loss: 19043.142942\n",
      "Epoch 48, loss: 19036.536610\n",
      "Epoch 49, loss: 19025.882709\n",
      "Epoch 50, loss: 19021.693876\n",
      "Epoch 51, loss: 19014.625430\n",
      "Epoch 52, loss: 19001.563310\n",
      "Epoch 53, loss: 19000.525963\n",
      "Epoch 54, loss: 18989.520030\n",
      "Epoch 55, loss: 18990.485188\n",
      "Epoch 56, loss: 18978.214053\n",
      "Epoch 57, loss: 18978.603949\n",
      "Epoch 58, loss: 18966.232996\n",
      "Epoch 59, loss: 18967.095228\n",
      "Epoch 60, loss: 18956.064283\n",
      "Epoch 61, loss: 18953.660502\n",
      "Epoch 62, loss: 18950.385949\n",
      "Epoch 63, loss: 18940.497491\n",
      "Epoch 64, loss: 18935.127625\n",
      "Epoch 65, loss: 18925.146242\n",
      "Epoch 66, loss: 18923.077658\n",
      "Epoch 67, loss: 18919.045811\n",
      "Epoch 68, loss: 18913.799487\n",
      "Epoch 69, loss: 18907.537192\n",
      "Epoch 70, loss: 18902.702531\n",
      "Epoch 71, loss: 18894.833300\n",
      "Epoch 72, loss: 18894.047167\n",
      "Epoch 73, loss: 18888.830598\n",
      "Epoch 74, loss: 18885.611533\n",
      "Epoch 75, loss: 18877.557469\n",
      "Epoch 76, loss: 18875.368754\n",
      "Epoch 77, loss: 18867.537998\n",
      "Epoch 78, loss: 18858.050539\n",
      "Epoch 79, loss: 18859.209344\n",
      "Epoch 80, loss: 18855.678912\n",
      "Epoch 81, loss: 18850.059779\n",
      "Epoch 82, loss: 18846.917783\n",
      "Epoch 83, loss: 18844.997875\n",
      "Epoch 84, loss: 18836.588162\n",
      "Epoch 85, loss: 18831.812527\n",
      "Epoch 86, loss: 18829.728592\n",
      "Epoch 87, loss: 18829.747027\n",
      "Epoch 88, loss: 18823.008665\n",
      "Epoch 89, loss: 18820.145970\n",
      "Epoch 90, loss: 18815.267657\n",
      "Epoch 91, loss: 18814.702559\n",
      "Epoch 92, loss: 18806.438726\n",
      "Epoch 93, loss: 18806.157781\n",
      "Epoch 94, loss: 18793.287689\n",
      "Epoch 95, loss: 18793.525659\n",
      "Epoch 96, loss: 18794.002026\n",
      "Epoch 97, loss: 18787.935227\n",
      "Epoch 98, loss: 18782.511942\n",
      "Epoch 99, loss: 18778.965191\n",
      "Epoch 100, loss: 18778.493742\n",
      "Epoch 101, loss: 18774.719545\n",
      "Epoch 102, loss: 18769.419140\n",
      "Epoch 103, loss: 18764.013444\n",
      "Epoch 104, loss: 18764.154322\n",
      "Epoch 105, loss: 18756.279688\n",
      "Epoch 106, loss: 18755.416544\n",
      "Epoch 107, loss: 18749.095238\n",
      "Epoch 108, loss: 18747.695023\n",
      "Epoch 109, loss: 18750.934746\n",
      "Epoch 110, loss: 18740.066644\n",
      "Epoch 111, loss: 18742.461740\n",
      "Epoch 112, loss: 18731.512601\n",
      "Epoch 113, loss: 18730.107132\n",
      "Epoch 114, loss: 18733.728179\n",
      "Epoch 115, loss: 18727.797618\n",
      "Epoch 116, loss: 18719.957710\n",
      "Epoch 117, loss: 18709.704336\n",
      "Epoch 118, loss: 18716.240406\n",
      "Epoch 119, loss: 18716.241010\n",
      "Epoch 120, loss: 18707.708890\n",
      "Epoch 121, loss: 18712.279676\n",
      "Epoch 122, loss: 18701.405593\n",
      "Epoch 123, loss: 18696.963572\n",
      "Epoch 124, loss: 18699.062898\n",
      "Epoch 125, loss: 18699.324587\n",
      "Epoch 126, loss: 18691.323756\n",
      "Epoch 127, loss: 18690.516864\n",
      "Epoch 128, loss: 18687.741760\n",
      "Epoch 129, loss: 18686.192280\n",
      "Epoch 130, loss: 18682.666519\n",
      "Epoch 131, loss: 18675.500848\n",
      "Epoch 132, loss: 18677.057464\n",
      "Epoch 133, loss: 18674.832905\n",
      "Epoch 134, loss: 18672.105478\n",
      "Epoch 135, loss: 18671.550294\n",
      "Epoch 136, loss: 18668.610121\n",
      "Epoch 137, loss: 18667.422090\n",
      "Epoch 138, loss: 18654.881251\n",
      "Epoch 139, loss: 18658.265997\n",
      "Epoch 140, loss: 18655.558219\n",
      "Epoch 141, loss: 18647.732406\n",
      "Epoch 142, loss: 18648.086590\n",
      "Epoch 143, loss: 18648.718764\n",
      "Epoch 144, loss: 18642.686829\n",
      "Epoch 145, loss: 18638.509980\n",
      "Epoch 146, loss: 18633.826382\n",
      "Epoch 147, loss: 18634.107692\n",
      "Epoch 148, loss: 18629.738248\n",
      "Epoch 149, loss: 18629.681882\n",
      "Epoch 150, loss: 18625.754106\n",
      "Epoch 151, loss: 18627.661937\n",
      "Epoch 152, loss: 18626.223954\n",
      "Epoch 153, loss: 18612.680014\n",
      "Epoch 154, loss: 18611.155777\n",
      "Epoch 155, loss: 18622.333930\n",
      "Epoch 156, loss: 18616.768895\n",
      "Epoch 157, loss: 18609.641708\n",
      "Epoch 158, loss: 18612.066244\n",
      "Epoch 159, loss: 18604.673248\n",
      "Epoch 160, loss: 18606.398303\n",
      "Epoch 161, loss: 18602.075052\n",
      "Epoch 162, loss: 18593.459001\n",
      "Epoch 163, loss: 18596.798909\n",
      "Epoch 164, loss: 18590.315178\n",
      "Epoch 165, loss: 18589.560712\n",
      "Epoch 166, loss: 18588.194665\n",
      "Epoch 167, loss: 18586.645493\n",
      "Epoch 168, loss: 18580.831026\n",
      "Epoch 169, loss: 18583.576785\n",
      "Epoch 170, loss: 18578.103455\n",
      "Epoch 171, loss: 18580.714815\n",
      "Epoch 172, loss: 18581.387936\n",
      "Epoch 173, loss: 18573.760786\n",
      "Epoch 174, loss: 18566.655026\n",
      "Epoch 175, loss: 18565.600039\n",
      "Epoch 176, loss: 18561.992941\n",
      "Epoch 177, loss: 18561.158589\n",
      "Epoch 178, loss: 18560.185932\n",
      "Epoch 179, loss: 18555.221982\n",
      "Epoch 180, loss: 18561.942566\n",
      "Epoch 181, loss: 18555.259385\n",
      "Epoch 182, loss: 18555.541193\n",
      "Epoch 183, loss: 18555.399481\n",
      "Epoch 184, loss: 18553.054642\n",
      "Epoch 185, loss: 18553.636166\n",
      "Epoch 186, loss: 18542.543058\n",
      "Epoch 187, loss: 18547.000249\n",
      "Epoch 188, loss: 18546.662088\n",
      "Epoch 189, loss: 18540.452040\n",
      "Epoch 190, loss: 18536.589920\n",
      "Epoch 191, loss: 18535.777461\n",
      "Epoch 192, loss: 18535.659418\n",
      "Epoch 193, loss: 18531.854585\n",
      "Epoch 194, loss: 18531.701427\n",
      "Epoch 195, loss: 18523.233179\n",
      "Epoch 196, loss: 18523.325928\n",
      "Epoch 197, loss: 18520.945875\n",
      "Epoch 198, loss: 18520.087046\n",
      "Epoch 199, loss: 18520.414662\n",
      "0.0001 1e-05 0.244\n",
      "Epoch 0, loss: 20645.107777\n",
      "Epoch 1, loss: 20445.152632\n",
      "Epoch 2, loss: 20294.288560\n",
      "Epoch 3, loss: 20156.622829\n",
      "Epoch 4, loss: 20042.501589\n",
      "Epoch 5, loss: 19950.576271\n",
      "Epoch 6, loss: 19866.479382\n",
      "Epoch 7, loss: 19797.535738\n",
      "Epoch 8, loss: 19738.237281\n",
      "Epoch 9, loss: 19678.152514\n",
      "Epoch 10, loss: 19640.784351\n",
      "Epoch 11, loss: 19595.284615\n",
      "Epoch 12, loss: 19560.177257\n",
      "Epoch 13, loss: 19519.674682\n",
      "Epoch 14, loss: 19496.269473\n",
      "Epoch 15, loss: 19460.613882\n",
      "Epoch 16, loss: 19442.804360\n",
      "Epoch 17, loss: 19417.239609\n",
      "Epoch 18, loss: 19392.597486\n",
      "Epoch 19, loss: 19368.311714\n",
      "Epoch 20, loss: 19353.687696\n",
      "Epoch 21, loss: 19334.251944\n",
      "Epoch 22, loss: 19315.173590\n",
      "Epoch 23, loss: 19304.366813\n",
      "Epoch 24, loss: 19281.988838\n",
      "Epoch 25, loss: 19271.344600\n",
      "Epoch 26, loss: 19252.650976\n",
      "Epoch 27, loss: 19244.497996\n",
      "Epoch 28, loss: 19230.093314\n",
      "Epoch 29, loss: 19217.765304\n",
      "Epoch 30, loss: 19208.383873\n",
      "Epoch 31, loss: 19190.050926\n",
      "Epoch 32, loss: 19180.915538\n",
      "Epoch 33, loss: 19172.326723\n",
      "Epoch 34, loss: 19158.021911\n",
      "Epoch 35, loss: 19148.651253\n",
      "Epoch 36, loss: 19136.514881\n",
      "Epoch 37, loss: 19131.985400\n",
      "Epoch 38, loss: 19121.246909\n",
      "Epoch 39, loss: 19108.514149\n",
      "Epoch 40, loss: 19098.546786\n",
      "Epoch 41, loss: 19092.198305\n",
      "Epoch 42, loss: 19078.895786\n",
      "Epoch 43, loss: 19072.850821\n",
      "Epoch 44, loss: 19069.407674\n",
      "Epoch 45, loss: 19054.991465\n",
      "Epoch 46, loss: 19052.468180\n",
      "Epoch 47, loss: 19051.618858\n",
      "Epoch 48, loss: 19036.713115\n",
      "Epoch 49, loss: 19025.998046\n",
      "Epoch 50, loss: 19014.032951\n",
      "Epoch 51, loss: 19014.423773\n",
      "Epoch 52, loss: 19007.202981\n",
      "Epoch 53, loss: 19007.004115\n",
      "Epoch 54, loss: 18989.757301\n",
      "Epoch 55, loss: 18990.029673\n",
      "Epoch 56, loss: 18978.317497\n",
      "Epoch 57, loss: 18979.713900\n",
      "Epoch 58, loss: 18962.658243\n",
      "Epoch 59, loss: 18965.911279\n",
      "Epoch 60, loss: 18957.839777\n",
      "Epoch 61, loss: 18955.519494\n",
      "Epoch 62, loss: 18945.318857\n",
      "Epoch 63, loss: 18941.318915\n",
      "Epoch 64, loss: 18926.495807\n",
      "Epoch 65, loss: 18933.610770\n",
      "Epoch 66, loss: 18926.285367\n",
      "Epoch 67, loss: 18913.760514\n",
      "Epoch 68, loss: 18912.614556\n",
      "Epoch 69, loss: 18905.305720\n",
      "Epoch 70, loss: 18907.661276\n",
      "Epoch 71, loss: 18893.658590\n",
      "Epoch 72, loss: 18891.193630\n",
      "Epoch 73, loss: 18890.734041\n",
      "Epoch 74, loss: 18881.178686\n",
      "Epoch 75, loss: 18875.109846\n",
      "Epoch 76, loss: 18870.769868\n",
      "Epoch 77, loss: 18864.466745\n",
      "Epoch 78, loss: 18865.215258\n",
      "Epoch 79, loss: 18860.205280\n",
      "Epoch 80, loss: 18860.023338\n",
      "Epoch 81, loss: 18854.609425\n",
      "Epoch 82, loss: 18848.262890\n",
      "Epoch 83, loss: 18841.875239\n",
      "Epoch 84, loss: 18843.522084\n",
      "Epoch 85, loss: 18835.615578\n",
      "Epoch 86, loss: 18826.438794\n",
      "Epoch 87, loss: 18828.553378\n",
      "Epoch 88, loss: 18818.454177\n",
      "Epoch 89, loss: 18816.403621\n",
      "Epoch 90, loss: 18820.237512\n",
      "Epoch 91, loss: 18810.467183\n",
      "Epoch 92, loss: 18810.523417\n",
      "Epoch 93, loss: 18801.029763\n",
      "Epoch 94, loss: 18803.118311\n",
      "Epoch 95, loss: 18793.896828\n",
      "Epoch 96, loss: 18795.109693\n",
      "Epoch 97, loss: 18792.381507\n",
      "Epoch 98, loss: 18783.991084\n",
      "Epoch 99, loss: 18778.171142\n",
      "Epoch 100, loss: 18775.031346\n",
      "Epoch 101, loss: 18771.066907\n",
      "Epoch 102, loss: 18771.685104\n",
      "Epoch 103, loss: 18767.867444\n",
      "Epoch 104, loss: 18769.504813\n",
      "Epoch 105, loss: 18762.265068\n",
      "Epoch 106, loss: 18754.580747\n",
      "Epoch 107, loss: 18755.363688\n",
      "Epoch 108, loss: 18749.800444\n",
      "Epoch 109, loss: 18746.360720\n",
      "Epoch 110, loss: 18742.235799\n",
      "Epoch 111, loss: 18736.122411\n",
      "Epoch 112, loss: 18741.025523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113, loss: 18735.333335\n",
      "Epoch 114, loss: 18728.466331\n",
      "Epoch 115, loss: 18727.596613\n",
      "Epoch 116, loss: 18722.702876\n",
      "Epoch 117, loss: 18719.336569\n",
      "Epoch 118, loss: 18719.060131\n",
      "Epoch 119, loss: 18709.590425\n",
      "Epoch 120, loss: 18707.059072\n",
      "Epoch 121, loss: 18703.911200\n",
      "Epoch 122, loss: 18706.042808\n",
      "Epoch 123, loss: 18696.082882\n",
      "Epoch 124, loss: 18697.197155\n",
      "Epoch 125, loss: 18695.055933\n",
      "Epoch 126, loss: 18692.919510\n",
      "Epoch 127, loss: 18682.749643\n",
      "Epoch 128, loss: 18688.106962\n",
      "Epoch 129, loss: 18686.075456\n",
      "Epoch 130, loss: 18674.707844\n",
      "Epoch 131, loss: 18680.766043\n",
      "Epoch 132, loss: 18674.458585\n",
      "Epoch 133, loss: 18672.767326\n",
      "Epoch 134, loss: 18675.325542\n",
      "Epoch 135, loss: 18663.778297\n",
      "Epoch 136, loss: 18660.621702\n",
      "Epoch 137, loss: 18665.340995\n",
      "Epoch 138, loss: 18656.758504\n",
      "Epoch 139, loss: 18652.691895\n",
      "Epoch 140, loss: 18652.116330\n",
      "Epoch 141, loss: 18652.870424\n",
      "Epoch 142, loss: 18648.602884\n",
      "Epoch 143, loss: 18646.348149\n",
      "Epoch 144, loss: 18639.737159\n",
      "Epoch 145, loss: 18640.491606\n",
      "Epoch 146, loss: 18633.701457\n",
      "Epoch 147, loss: 18635.315437\n",
      "Epoch 148, loss: 18636.367114\n",
      "Epoch 149, loss: 18633.931827\n",
      "Epoch 150, loss: 18626.100445\n",
      "Epoch 151, loss: 18627.781529\n",
      "Epoch 152, loss: 18622.467665\n",
      "Epoch 153, loss: 18616.685362\n",
      "Epoch 154, loss: 18620.035284\n",
      "Epoch 155, loss: 18617.731963\n",
      "Epoch 156, loss: 18610.157358\n",
      "Epoch 157, loss: 18608.973322\n",
      "Epoch 158, loss: 18604.227898\n",
      "Epoch 159, loss: 18601.353213\n",
      "Epoch 160, loss: 18605.889397\n",
      "Epoch 161, loss: 18600.664306\n",
      "Epoch 162, loss: 18597.500218\n",
      "Epoch 163, loss: 18595.612538\n",
      "Epoch 164, loss: 18595.778713\n",
      "Epoch 165, loss: 18592.159375\n",
      "Epoch 166, loss: 18584.669613\n",
      "Epoch 167, loss: 18593.342845\n",
      "Epoch 168, loss: 18582.183971\n",
      "Epoch 169, loss: 18577.448858\n",
      "Epoch 170, loss: 18581.658038\n",
      "Epoch 171, loss: 18575.686215\n",
      "Epoch 172, loss: 18569.361005\n",
      "Epoch 173, loss: 18575.972571\n",
      "Epoch 174, loss: 18574.195910\n",
      "Epoch 175, loss: 18567.193394\n",
      "Epoch 176, loss: 18564.438557\n",
      "Epoch 177, loss: 18563.925272\n",
      "Epoch 178, loss: 18560.849390\n",
      "Epoch 179, loss: 18557.146004\n",
      "Epoch 180, loss: 18554.620001\n",
      "Epoch 181, loss: 18558.942538\n",
      "Epoch 182, loss: 18552.063354\n",
      "Epoch 183, loss: 18547.446203\n",
      "Epoch 184, loss: 18545.289210\n",
      "Epoch 185, loss: 18550.564487\n",
      "Epoch 186, loss: 18545.800494\n",
      "Epoch 187, loss: 18542.574276\n",
      "Epoch 188, loss: 18538.591555\n",
      "Epoch 189, loss: 18541.994224\n",
      "Epoch 190, loss: 18533.679423\n",
      "Epoch 191, loss: 18538.972183\n",
      "Epoch 192, loss: 18533.732701\n",
      "Epoch 193, loss: 18533.941235\n",
      "Epoch 194, loss: 18525.560487\n",
      "Epoch 195, loss: 18521.257261\n",
      "Epoch 196, loss: 18522.524349\n",
      "Epoch 197, loss: 18520.725599\n",
      "Epoch 198, loss: 18516.660846\n",
      "Epoch 199, loss: 18522.101808\n",
      "0.0001 1e-06 0.254\n",
      "Epoch 0, loss: 20714.873841\n",
      "Epoch 1, loss: 20689.020148\n",
      "Epoch 2, loss: 20665.057664\n",
      "Epoch 3, loss: 20641.596220\n",
      "Epoch 4, loss: 20619.266578\n",
      "Epoch 5, loss: 20597.397008\n",
      "Epoch 6, loss: 20576.206215\n",
      "Epoch 7, loss: 20555.127659\n",
      "Epoch 8, loss: 20534.387274\n",
      "Epoch 9, loss: 20515.536912\n",
      "Epoch 10, loss: 20495.244532\n",
      "Epoch 11, loss: 20476.383552\n",
      "Epoch 12, loss: 20458.037458\n",
      "Epoch 13, loss: 20439.406561\n",
      "Epoch 14, loss: 20421.473677\n",
      "Epoch 15, loss: 20404.832584\n",
      "Epoch 16, loss: 20386.703944\n",
      "Epoch 17, loss: 20370.565075\n",
      "Epoch 18, loss: 20353.451777\n",
      "Epoch 19, loss: 20337.848377\n",
      "Epoch 20, loss: 20321.425953\n",
      "Epoch 21, loss: 20305.517451\n",
      "Epoch 22, loss: 20290.806791\n",
      "Epoch 23, loss: 20275.351545\n",
      "Epoch 24, loss: 20260.182957\n",
      "Epoch 25, loss: 20246.009149\n",
      "Epoch 26, loss: 20232.141110\n",
      "Epoch 27, loss: 20217.834529\n",
      "Epoch 28, loss: 20204.617693\n",
      "Epoch 29, loss: 20190.903941\n",
      "Epoch 30, loss: 20177.701724\n",
      "Epoch 31, loss: 20164.406781\n",
      "Epoch 32, loss: 20152.430403\n",
      "Epoch 33, loss: 20139.251023\n",
      "Epoch 34, loss: 20127.012848\n",
      "Epoch 35, loss: 20114.694230\n",
      "Epoch 36, loss: 20102.640071\n",
      "Epoch 37, loss: 20091.596694\n",
      "Epoch 38, loss: 20079.220442\n",
      "Epoch 39, loss: 20068.040549\n",
      "Epoch 40, loss: 20056.974972\n",
      "Epoch 41, loss: 20045.744660\n",
      "Epoch 42, loss: 20035.253625\n",
      "Epoch 43, loss: 20023.941792\n",
      "Epoch 44, loss: 20014.042749\n",
      "Epoch 45, loss: 20004.148617\n",
      "Epoch 46, loss: 19993.947175\n",
      "Epoch 47, loss: 19983.828324\n",
      "Epoch 48, loss: 19973.618418\n",
      "Epoch 49, loss: 19964.422369\n",
      "Epoch 50, loss: 19954.790747\n",
      "Epoch 51, loss: 19945.402343\n",
      "Epoch 52, loss: 19936.361962\n",
      "Epoch 53, loss: 19927.058304\n",
      "Epoch 54, loss: 19918.684519\n",
      "Epoch 55, loss: 19909.537507\n",
      "Epoch 56, loss: 19900.733422\n",
      "Epoch 57, loss: 19892.363859\n",
      "Epoch 58, loss: 19883.928444\n",
      "Epoch 59, loss: 19875.962311\n",
      "Epoch 60, loss: 19867.973133\n",
      "Epoch 61, loss: 19859.944571\n",
      "Epoch 62, loss: 19852.262987\n",
      "Epoch 63, loss: 19844.509441\n",
      "Epoch 64, loss: 19837.020949\n",
      "Epoch 65, loss: 19829.380193\n",
      "Epoch 66, loss: 19821.551709\n",
      "Epoch 67, loss: 19814.971764\n",
      "Epoch 68, loss: 19807.267702\n",
      "Epoch 69, loss: 19800.463147\n",
      "Epoch 70, loss: 19793.304030\n",
      "Epoch 71, loss: 19786.219624\n",
      "Epoch 72, loss: 19780.006679\n",
      "Epoch 73, loss: 19772.676801\n",
      "Epoch 74, loss: 19766.553205\n",
      "Epoch 75, loss: 19759.678823\n",
      "Epoch 76, loss: 19754.195064\n",
      "Epoch 77, loss: 19747.580549\n",
      "Epoch 78, loss: 19740.928577\n",
      "Epoch 79, loss: 19735.364327\n",
      "Epoch 80, loss: 19728.629125\n",
      "Epoch 81, loss: 19723.106834\n",
      "Epoch 82, loss: 19717.045149\n",
      "Epoch 83, loss: 19710.914292\n",
      "Epoch 84, loss: 19705.700854\n",
      "Epoch 85, loss: 19699.762593\n",
      "Epoch 86, loss: 19694.136516\n",
      "Epoch 87, loss: 19689.267400\n",
      "Epoch 88, loss: 19683.815772\n",
      "Epoch 89, loss: 19677.636531\n",
      "Epoch 90, loss: 19672.815515\n",
      "Epoch 91, loss: 19667.313817\n",
      "Epoch 92, loss: 19663.029739\n",
      "Epoch 93, loss: 19657.837917\n",
      "Epoch 94, loss: 19652.822646\n",
      "Epoch 95, loss: 19647.586923\n",
      "Epoch 96, loss: 19642.914989\n",
      "Epoch 97, loss: 19637.752031\n",
      "Epoch 98, loss: 19633.187865\n",
      "Epoch 99, loss: 19628.433757\n",
      "Epoch 100, loss: 19624.212812\n",
      "Epoch 101, loss: 19618.963714\n",
      "Epoch 102, loss: 19614.783320\n",
      "Epoch 103, loss: 19610.422214\n",
      "Epoch 104, loss: 19605.728706\n",
      "Epoch 105, loss: 19601.228072\n",
      "Epoch 106, loss: 19596.932536\n",
      "Epoch 107, loss: 19592.540635\n",
      "Epoch 108, loss: 19588.581121\n",
      "Epoch 109, loss: 19584.424707\n",
      "Epoch 110, loss: 19580.276751\n",
      "Epoch 111, loss: 19576.633256\n",
      "Epoch 112, loss: 19571.919578\n",
      "Epoch 113, loss: 19567.967881\n",
      "Epoch 114, loss: 19564.324022\n",
      "Epoch 115, loss: 19560.453736\n",
      "Epoch 116, loss: 19556.722233\n",
      "Epoch 117, loss: 19552.187191\n",
      "Epoch 118, loss: 19548.985657\n",
      "Epoch 119, loss: 19545.063712\n",
      "Epoch 120, loss: 19541.432782\n",
      "Epoch 121, loss: 19537.775844\n",
      "Epoch 122, loss: 19534.128732\n",
      "Epoch 123, loss: 19530.260835\n",
      "Epoch 124, loss: 19526.824321\n",
      "Epoch 125, loss: 19523.051979\n",
      "Epoch 126, loss: 19520.171047\n",
      "Epoch 127, loss: 19516.343922\n",
      "Epoch 128, loss: 19513.091851\n",
      "Epoch 129, loss: 19510.062623\n",
      "Epoch 130, loss: 19506.573181\n",
      "Epoch 131, loss: 19503.149344\n",
      "Epoch 132, loss: 19499.465985\n",
      "Epoch 133, loss: 19496.783675\n",
      "Epoch 134, loss: 19493.189156\n",
      "Epoch 135, loss: 19489.769144\n",
      "Epoch 136, loss: 19487.386560\n",
      "Epoch 137, loss: 19483.797703\n",
      "Epoch 138, loss: 19480.540192\n",
      "Epoch 139, loss: 19477.455342\n",
      "Epoch 140, loss: 19474.219598\n",
      "Epoch 141, loss: 19471.955577\n",
      "Epoch 142, loss: 19468.508301\n",
      "Epoch 143, loss: 19466.109755\n",
      "Epoch 144, loss: 19462.891202\n",
      "Epoch 145, loss: 19460.100274\n",
      "Epoch 146, loss: 19457.031582\n",
      "Epoch 147, loss: 19454.071522\n",
      "Epoch 148, loss: 19451.282021\n",
      "Epoch 149, loss: 19448.155586\n",
      "Epoch 150, loss: 19445.568563\n",
      "Epoch 151, loss: 19443.053594\n",
      "Epoch 152, loss: 19439.715194\n",
      "Epoch 153, loss: 19437.345835\n",
      "Epoch 154, loss: 19434.690592\n",
      "Epoch 155, loss: 19432.138954\n",
      "Epoch 156, loss: 19429.423943\n",
      "Epoch 157, loss: 19427.089759\n",
      "Epoch 158, loss: 19423.967488\n",
      "Epoch 159, loss: 19421.841382\n",
      "Epoch 160, loss: 19419.126826\n",
      "Epoch 161, loss: 19416.959009\n",
      "Epoch 162, loss: 19414.165328\n",
      "Epoch 163, loss: 19411.032404\n",
      "Epoch 164, loss: 19408.866619\n",
      "Epoch 165, loss: 19406.739714\n",
      "Epoch 166, loss: 19403.850698\n",
      "Epoch 167, loss: 19401.345685\n",
      "Epoch 168, loss: 19399.013453\n",
      "Epoch 169, loss: 19397.235234\n",
      "Epoch 170, loss: 19394.336694\n",
      "Epoch 171, loss: 19392.515901\n",
      "Epoch 172, loss: 19389.487861\n",
      "Epoch 173, loss: 19387.671351\n",
      "Epoch 174, loss: 19385.224066\n",
      "Epoch 175, loss: 19382.482948\n",
      "Epoch 176, loss: 19380.501767\n",
      "Epoch 177, loss: 19378.418425\n",
      "Epoch 178, loss: 19376.158556\n",
      "Epoch 179, loss: 19373.598404\n",
      "Epoch 180, loss: 19371.944774\n",
      "Epoch 181, loss: 19369.618708\n",
      "Epoch 182, loss: 19366.595288\n",
      "Epoch 183, loss: 19365.153952\n",
      "Epoch 184, loss: 19362.813354\n",
      "Epoch 185, loss: 19360.228290\n",
      "Epoch 186, loss: 19358.566618\n",
      "Epoch 187, loss: 19356.347915\n",
      "Epoch 188, loss: 19354.396171\n",
      "Epoch 189, loss: 19351.877720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, loss: 19350.393411\n",
      "Epoch 191, loss: 19348.018223\n",
      "Epoch 192, loss: 19346.087247\n",
      "Epoch 193, loss: 19343.970942\n",
      "Epoch 194, loss: 19341.915445\n",
      "Epoch 195, loss: 19340.084258\n",
      "Epoch 196, loss: 19338.065561\n",
      "Epoch 197, loss: 19335.906796\n",
      "Epoch 198, loss: 19333.977538\n",
      "Epoch 199, loss: 19331.859299\n",
      "1e-05 0.0001 0.231\n",
      "Epoch 0, loss: 20714.435938\n",
      "Epoch 1, loss: 20688.456211\n",
      "Epoch 2, loss: 20664.011852\n",
      "Epoch 3, loss: 20640.910620\n",
      "Epoch 4, loss: 20618.412587\n",
      "Epoch 5, loss: 20597.096391\n",
      "Epoch 6, loss: 20575.023958\n",
      "Epoch 7, loss: 20554.934627\n",
      "Epoch 8, loss: 20533.985073\n",
      "Epoch 9, loss: 20514.517249\n",
      "Epoch 10, loss: 20494.722583\n",
      "Epoch 11, loss: 20475.808699\n",
      "Epoch 12, loss: 20456.834494\n",
      "Epoch 13, loss: 20438.615456\n",
      "Epoch 14, loss: 20420.519398\n",
      "Epoch 15, loss: 20403.267497\n",
      "Epoch 16, loss: 20386.330767\n",
      "Epoch 17, loss: 20369.642620\n",
      "Epoch 18, loss: 20353.060763\n",
      "Epoch 19, loss: 20336.260840\n",
      "Epoch 20, loss: 20320.648682\n",
      "Epoch 21, loss: 20305.190231\n",
      "Epoch 22, loss: 20289.719387\n",
      "Epoch 23, loss: 20274.444956\n",
      "Epoch 24, loss: 20260.051997\n",
      "Epoch 25, loss: 20245.628375\n",
      "Epoch 26, loss: 20231.275248\n",
      "Epoch 27, loss: 20217.071367\n",
      "Epoch 28, loss: 20203.408905\n",
      "Epoch 29, loss: 20189.999923\n",
      "Epoch 30, loss: 20176.690674\n",
      "Epoch 31, loss: 20163.851038\n",
      "Epoch 32, loss: 20151.185909\n",
      "Epoch 33, loss: 20138.318228\n",
      "Epoch 34, loss: 20126.075556\n",
      "Epoch 35, loss: 20113.510212\n",
      "Epoch 36, loss: 20101.559641\n",
      "Epoch 37, loss: 20090.033061\n",
      "Epoch 38, loss: 20078.708985\n",
      "Epoch 39, loss: 20067.099696\n",
      "Epoch 40, loss: 20055.966652\n",
      "Epoch 41, loss: 20044.891990\n",
      "Epoch 42, loss: 20034.338524\n",
      "Epoch 43, loss: 20023.475492\n",
      "Epoch 44, loss: 20013.202251\n",
      "Epoch 45, loss: 20002.999179\n",
      "Epoch 46, loss: 19992.441749\n",
      "Epoch 47, loss: 19983.034483\n",
      "Epoch 48, loss: 19972.499198\n",
      "Epoch 49, loss: 19963.570984\n",
      "Epoch 50, loss: 19953.575313\n",
      "Epoch 51, loss: 19944.771917\n",
      "Epoch 52, loss: 19935.587191\n",
      "Epoch 53, loss: 19926.586135\n",
      "Epoch 54, loss: 19917.239124\n",
      "Epoch 55, loss: 19908.856467\n",
      "Epoch 56, loss: 19899.957410\n",
      "Epoch 57, loss: 19891.304783\n",
      "Epoch 58, loss: 19883.553745\n",
      "Epoch 59, loss: 19875.290072\n",
      "Epoch 60, loss: 19866.839697\n",
      "Epoch 61, loss: 19858.685791\n",
      "Epoch 62, loss: 19851.619627\n",
      "Epoch 63, loss: 19843.287683\n",
      "Epoch 64, loss: 19835.732035\n",
      "Epoch 65, loss: 19827.988026\n",
      "Epoch 66, loss: 19820.780679\n",
      "Epoch 67, loss: 19813.598643\n",
      "Epoch 68, loss: 19805.936719\n",
      "Epoch 69, loss: 19799.260926\n",
      "Epoch 70, loss: 19792.406617\n",
      "Epoch 71, loss: 19785.243489\n",
      "Epoch 72, loss: 19778.358405\n",
      "Epoch 73, loss: 19771.871427\n",
      "Epoch 74, loss: 19765.544073\n",
      "Epoch 75, loss: 19759.089856\n",
      "Epoch 76, loss: 19753.205724\n",
      "Epoch 77, loss: 19746.348934\n",
      "Epoch 78, loss: 19740.439781\n",
      "Epoch 79, loss: 19734.576714\n",
      "Epoch 80, loss: 19727.965314\n",
      "Epoch 81, loss: 19721.669639\n",
      "Epoch 82, loss: 19715.775581\n",
      "Epoch 83, loss: 19710.178297\n",
      "Epoch 84, loss: 19704.732694\n",
      "Epoch 85, loss: 19699.065685\n",
      "Epoch 86, loss: 19693.027143\n",
      "Epoch 87, loss: 19687.388593\n",
      "Epoch 88, loss: 19682.777006\n",
      "Epoch 89, loss: 19676.911819\n",
      "Epoch 90, loss: 19671.589609\n",
      "Epoch 91, loss: 19666.936506\n",
      "Epoch 92, loss: 19661.742760\n",
      "Epoch 93, loss: 19656.291003\n",
      "Epoch 94, loss: 19651.216622\n",
      "Epoch 95, loss: 19646.161461\n",
      "Epoch 96, loss: 19641.469956\n",
      "Epoch 97, loss: 19636.528117\n",
      "Epoch 98, loss: 19631.852242\n",
      "Epoch 99, loss: 19627.452280\n",
      "Epoch 100, loss: 19622.959845\n",
      "Epoch 101, loss: 19617.615895\n",
      "Epoch 102, loss: 19613.497788\n",
      "Epoch 103, loss: 19609.124835\n",
      "Epoch 104, loss: 19604.429321\n",
      "Epoch 105, loss: 19600.344717\n",
      "Epoch 106, loss: 19596.097483\n",
      "Epoch 107, loss: 19592.062713\n",
      "Epoch 108, loss: 19587.535755\n",
      "Epoch 109, loss: 19583.092965\n",
      "Epoch 110, loss: 19579.380399\n",
      "Epoch 111, loss: 19575.254081\n",
      "Epoch 112, loss: 19570.487803\n",
      "Epoch 113, loss: 19566.861554\n",
      "Epoch 114, loss: 19563.257536\n",
      "Epoch 115, loss: 19559.269412\n",
      "Epoch 116, loss: 19555.404856\n",
      "Epoch 117, loss: 19550.999235\n",
      "Epoch 118, loss: 19547.424554\n",
      "Epoch 119, loss: 19544.235909\n",
      "Epoch 120, loss: 19539.994288\n",
      "Epoch 121, loss: 19536.382119\n",
      "Epoch 122, loss: 19533.062025\n",
      "Epoch 123, loss: 19529.052069\n",
      "Epoch 124, loss: 19526.047736\n",
      "Epoch 125, loss: 19522.030138\n",
      "Epoch 126, loss: 19519.045580\n",
      "Epoch 127, loss: 19515.530977\n",
      "Epoch 128, loss: 19512.104380\n",
      "Epoch 129, loss: 19508.416423\n",
      "Epoch 130, loss: 19505.048162\n",
      "Epoch 131, loss: 19501.792178\n",
      "Epoch 132, loss: 19498.257291\n",
      "Epoch 133, loss: 19495.338025\n",
      "Epoch 134, loss: 19492.488837\n",
      "Epoch 135, loss: 19489.144396\n",
      "Epoch 136, loss: 19485.604858\n",
      "Epoch 137, loss: 19482.865991\n",
      "Epoch 138, loss: 19479.254159\n",
      "Epoch 139, loss: 19476.548712\n",
      "Epoch 140, loss: 19473.504172\n",
      "Epoch 141, loss: 19470.601429\n",
      "Epoch 142, loss: 19467.387913\n",
      "Epoch 143, loss: 19464.095080\n",
      "Epoch 144, loss: 19461.366060\n",
      "Epoch 145, loss: 19458.777225\n",
      "Epoch 146, loss: 19455.618891\n",
      "Epoch 147, loss: 19452.949783\n",
      "Epoch 148, loss: 19450.405683\n",
      "Epoch 149, loss: 19447.938509\n",
      "Epoch 150, loss: 19444.860385\n",
      "Epoch 151, loss: 19442.024763\n",
      "Epoch 152, loss: 19439.202217\n",
      "Epoch 153, loss: 19437.036400\n",
      "Epoch 154, loss: 19433.608885\n",
      "Epoch 155, loss: 19431.230538\n",
      "Epoch 156, loss: 19428.418084\n",
      "Epoch 157, loss: 19426.106283\n",
      "Epoch 158, loss: 19423.419415\n",
      "Epoch 159, loss: 19420.441687\n",
      "Epoch 160, loss: 19417.844073\n",
      "Epoch 161, loss: 19415.296675\n",
      "Epoch 162, loss: 19412.550508\n",
      "Epoch 163, loss: 19410.710994\n",
      "Epoch 164, loss: 19408.022999\n",
      "Epoch 165, loss: 19405.576467\n",
      "Epoch 166, loss: 19402.842954\n",
      "Epoch 167, loss: 19400.852509\n",
      "Epoch 168, loss: 19398.384721\n",
      "Epoch 169, loss: 19395.779389\n",
      "Epoch 170, loss: 19392.900361\n",
      "Epoch 171, loss: 19390.890125\n",
      "Epoch 172, loss: 19389.077132\n",
      "Epoch 173, loss: 19386.234394\n",
      "Epoch 174, loss: 19384.182115\n",
      "Epoch 175, loss: 19381.562279\n",
      "Epoch 176, loss: 19379.281041\n",
      "Epoch 177, loss: 19377.222755\n",
      "Epoch 178, loss: 19374.893444\n",
      "Epoch 179, loss: 19372.581773\n",
      "Epoch 180, loss: 19370.428849\n",
      "Epoch 181, loss: 19368.266429\n",
      "Epoch 182, loss: 19366.096386\n",
      "Epoch 183, loss: 19364.036346\n",
      "Epoch 184, loss: 19361.746691\n",
      "Epoch 185, loss: 19359.577738\n",
      "Epoch 186, loss: 19357.747667\n",
      "Epoch 187, loss: 19354.999182\n",
      "Epoch 188, loss: 19353.679104\n",
      "Epoch 189, loss: 19351.087354\n",
      "Epoch 190, loss: 19349.557997\n",
      "Epoch 191, loss: 19347.484645\n",
      "Epoch 192, loss: 19344.691270\n",
      "Epoch 193, loss: 19342.472500\n",
      "Epoch 194, loss: 19341.743482\n",
      "Epoch 195, loss: 19339.232501\n",
      "Epoch 196, loss: 19336.640129\n",
      "Epoch 197, loss: 19334.970480\n",
      "Epoch 198, loss: 19333.277414\n",
      "Epoch 199, loss: 19330.704488\n",
      "1e-05 1e-05 0.232\n",
      "Epoch 0, loss: 20714.289382\n",
      "Epoch 1, loss: 20688.380170\n",
      "Epoch 2, loss: 20664.260468\n",
      "Epoch 3, loss: 20641.872881\n",
      "Epoch 4, loss: 20618.930301\n",
      "Epoch 5, loss: 20597.181663\n",
      "Epoch 6, loss: 20576.050879\n",
      "Epoch 7, loss: 20554.699807\n",
      "Epoch 8, loss: 20534.667429\n",
      "Epoch 9, loss: 20514.690778\n",
      "Epoch 10, loss: 20495.552202\n",
      "Epoch 11, loss: 20476.437459\n",
      "Epoch 12, loss: 20457.819453\n",
      "Epoch 13, loss: 20439.580678\n",
      "Epoch 14, loss: 20421.963043\n",
      "Epoch 15, loss: 20404.056150\n",
      "Epoch 16, loss: 20387.474371\n",
      "Epoch 17, loss: 20369.714832\n",
      "Epoch 18, loss: 20353.154084\n",
      "Epoch 19, loss: 20337.322216\n",
      "Epoch 20, loss: 20321.627985\n",
      "Epoch 21, loss: 20305.703189\n",
      "Epoch 22, loss: 20290.742768\n",
      "Epoch 23, loss: 20275.208320\n",
      "Epoch 24, loss: 20260.443848\n",
      "Epoch 25, loss: 20246.363465\n",
      "Epoch 26, loss: 20232.072221\n",
      "Epoch 27, loss: 20217.864346\n",
      "Epoch 28, loss: 20204.117164\n",
      "Epoch 29, loss: 20190.769872\n",
      "Epoch 30, loss: 20177.487905\n",
      "Epoch 31, loss: 20164.832603\n",
      "Epoch 32, loss: 20151.903653\n",
      "Epoch 33, loss: 20139.508227\n",
      "Epoch 34, loss: 20126.706579\n",
      "Epoch 35, loss: 20114.583295\n",
      "Epoch 36, loss: 20102.432480\n",
      "Epoch 37, loss: 20090.905486\n",
      "Epoch 38, loss: 20079.295795\n",
      "Epoch 39, loss: 20067.407467\n",
      "Epoch 40, loss: 20056.507085\n",
      "Epoch 41, loss: 20045.499342\n",
      "Epoch 42, loss: 20034.936686\n",
      "Epoch 43, loss: 20023.866546\n",
      "Epoch 44, loss: 20013.729734\n",
      "Epoch 45, loss: 20003.497143\n",
      "Epoch 46, loss: 19993.299079\n",
      "Epoch 47, loss: 19983.508371\n",
      "Epoch 48, loss: 19973.561539\n",
      "Epoch 49, loss: 19963.706860\n",
      "Epoch 50, loss: 19954.359969\n",
      "Epoch 51, loss: 19944.947787\n",
      "Epoch 52, loss: 19935.573251\n",
      "Epoch 53, loss: 19926.514301\n",
      "Epoch 54, loss: 19918.045260\n",
      "Epoch 55, loss: 19909.592470\n",
      "Epoch 56, loss: 19900.668366\n",
      "Epoch 57, loss: 19892.168111\n",
      "Epoch 58, loss: 19883.995464\n",
      "Epoch 59, loss: 19875.640238\n",
      "Epoch 60, loss: 19867.765894\n",
      "Epoch 61, loss: 19859.365168\n",
      "Epoch 62, loss: 19851.627459\n",
      "Epoch 63, loss: 19844.157369\n",
      "Epoch 64, loss: 19836.178821\n",
      "Epoch 65, loss: 19828.624964\n",
      "Epoch 66, loss: 19820.932400\n",
      "Epoch 67, loss: 19814.005713\n",
      "Epoch 68, loss: 19806.747261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, loss: 19799.900375\n",
      "Epoch 70, loss: 19792.794763\n",
      "Epoch 71, loss: 19786.244877\n",
      "Epoch 72, loss: 19778.849663\n",
      "Epoch 73, loss: 19772.188392\n",
      "Epoch 74, loss: 19766.194995\n",
      "Epoch 75, loss: 19759.266750\n",
      "Epoch 76, loss: 19752.938985\n",
      "Epoch 77, loss: 19747.185769\n",
      "Epoch 78, loss: 19740.398119\n",
      "Epoch 79, loss: 19733.986696\n",
      "Epoch 80, loss: 19728.764545\n",
      "Epoch 81, loss: 19722.443030\n",
      "Epoch 82, loss: 19716.388142\n",
      "Epoch 83, loss: 19710.613776\n",
      "Epoch 84, loss: 19704.929480\n",
      "Epoch 85, loss: 19699.406267\n",
      "Epoch 86, loss: 19694.266768\n",
      "Epoch 87, loss: 19688.159972\n",
      "Epoch 88, loss: 19683.205372\n",
      "Epoch 89, loss: 19677.888216\n",
      "Epoch 90, loss: 19672.314187\n",
      "Epoch 91, loss: 19666.916545\n",
      "Epoch 92, loss: 19662.025516\n",
      "Epoch 93, loss: 19657.078306\n",
      "Epoch 94, loss: 19652.335276\n",
      "Epoch 95, loss: 19646.773029\n",
      "Epoch 96, loss: 19642.146672\n",
      "Epoch 97, loss: 19637.459346\n",
      "Epoch 98, loss: 19632.569234\n",
      "Epoch 99, loss: 19627.923446\n",
      "Epoch 100, loss: 19623.229555\n",
      "Epoch 101, loss: 19618.150020\n",
      "Epoch 102, loss: 19614.177490\n",
      "Epoch 103, loss: 19609.660955\n",
      "Epoch 104, loss: 19605.109907\n",
      "Epoch 105, loss: 19600.696684\n",
      "Epoch 106, loss: 19596.812984\n",
      "Epoch 107, loss: 19592.164646\n",
      "Epoch 108, loss: 19587.832107\n",
      "Epoch 109, loss: 19583.810695\n",
      "Epoch 110, loss: 19579.418642\n",
      "Epoch 111, loss: 19575.695189\n",
      "Epoch 112, loss: 19571.548611\n",
      "Epoch 113, loss: 19567.605865\n",
      "Epoch 114, loss: 19563.899941\n",
      "Epoch 115, loss: 19559.766286\n",
      "Epoch 116, loss: 19555.658576\n",
      "Epoch 117, loss: 19552.040449\n",
      "Epoch 118, loss: 19548.350830\n",
      "Epoch 119, loss: 19544.586777\n",
      "Epoch 120, loss: 19540.466336\n",
      "Epoch 121, loss: 19536.994633\n",
      "Epoch 122, loss: 19533.172035\n",
      "Epoch 123, loss: 19529.743581\n",
      "Epoch 124, loss: 19526.128335\n",
      "Epoch 125, loss: 19522.630481\n",
      "Epoch 126, loss: 19519.464845\n",
      "Epoch 127, loss: 19515.623904\n",
      "Epoch 128, loss: 19512.111617\n",
      "Epoch 129, loss: 19509.216270\n",
      "Epoch 130, loss: 19505.233644\n",
      "Epoch 131, loss: 19502.598253\n",
      "Epoch 132, loss: 19499.143653\n",
      "Epoch 133, loss: 19495.741687\n",
      "Epoch 134, loss: 19492.369363\n",
      "Epoch 135, loss: 19489.126029\n",
      "Epoch 136, loss: 19486.759040\n",
      "Epoch 137, loss: 19482.953960\n",
      "Epoch 138, loss: 19480.146717\n",
      "Epoch 139, loss: 19477.323742\n",
      "Epoch 140, loss: 19473.644573\n",
      "Epoch 141, loss: 19471.110412\n",
      "Epoch 142, loss: 19468.537693\n",
      "Epoch 143, loss: 19465.051037\n",
      "Epoch 144, loss: 19461.907003\n",
      "Epoch 145, loss: 19459.203321\n",
      "Epoch 146, loss: 19456.163578\n",
      "Epoch 147, loss: 19453.878080\n",
      "Epoch 148, loss: 19451.333327\n",
      "Epoch 149, loss: 19447.628802\n",
      "Epoch 150, loss: 19445.111900\n",
      "Epoch 151, loss: 19442.229005\n",
      "Epoch 152, loss: 19439.704165\n",
      "Epoch 153, loss: 19436.812910\n",
      "Epoch 154, loss: 19434.144759\n",
      "Epoch 155, loss: 19431.599527\n",
      "Epoch 156, loss: 19429.133031\n",
      "Epoch 157, loss: 19425.878230\n",
      "Epoch 158, loss: 19423.217755\n",
      "Epoch 159, loss: 19420.974464\n",
      "Epoch 160, loss: 19417.971779\n",
      "Epoch 161, loss: 19415.910816\n",
      "Epoch 162, loss: 19413.403992\n",
      "Epoch 163, loss: 19410.810801\n",
      "Epoch 164, loss: 19408.408135\n",
      "Epoch 165, loss: 19405.753273\n",
      "Epoch 166, loss: 19403.279869\n",
      "Epoch 167, loss: 19400.941492\n",
      "Epoch 168, loss: 19398.754470\n",
      "Epoch 169, loss: 19396.610330\n",
      "Epoch 170, loss: 19393.814825\n",
      "Epoch 171, loss: 19391.516634\n",
      "Epoch 172, loss: 19388.751192\n",
      "Epoch 173, loss: 19386.942532\n",
      "Epoch 174, loss: 19384.405291\n",
      "Epoch 175, loss: 19382.673576\n",
      "Epoch 176, loss: 19379.423509\n",
      "Epoch 177, loss: 19377.558505\n",
      "Epoch 178, loss: 19375.044358\n",
      "Epoch 179, loss: 19373.156000\n",
      "Epoch 180, loss: 19371.261029\n",
      "Epoch 181, loss: 19369.206374\n",
      "Epoch 182, loss: 19366.488377\n",
      "Epoch 183, loss: 19364.529649\n",
      "Epoch 184, loss: 19362.611729\n",
      "Epoch 185, loss: 19360.479080\n",
      "Epoch 186, loss: 19357.786831\n",
      "Epoch 187, loss: 19355.667860\n",
      "Epoch 188, loss: 19354.073303\n",
      "Epoch 189, loss: 19351.795486\n",
      "Epoch 190, loss: 19349.559825\n",
      "Epoch 191, loss: 19347.623710\n",
      "Epoch 192, loss: 19345.531499\n",
      "Epoch 193, loss: 19343.247343\n",
      "Epoch 194, loss: 19341.112101\n",
      "Epoch 195, loss: 19339.835481\n",
      "Epoch 196, loss: 19337.254888\n",
      "Epoch 197, loss: 19335.432367\n",
      "Epoch 198, loss: 19333.411239\n",
      "Epoch 199, loss: 19331.497866\n",
      "1e-05 1e-06 0.229\n",
      "best validation accuracy achieved: 0.254000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg_str in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg_str)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy is None or best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "        print(lr, reg_str, accuracy)\n",
    "            \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
