{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание рассчитывалось в google colab\n",
    "#https://colab.research.google.com/drive/1QizL00qjhmQTD2tBM5Mm83aWI-eibfWN#scrollTo=BUvN94eFvcQt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIimjfL_vcPQ"
   },
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch\n",
    "!pip3 -qq install bokeh\n",
    "!pip3 -qq install gensim\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "TiA2dGmgF1rW",
    "outputId": "ac8a983a-95a9-45ef-c29e-d742b404668b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "QstS4NO0L97c",
    "outputId": "7f49c194-6438-496d-dc9d-feef401e2b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "xTai8Ta0lgwL",
    "outputId": "8e1944d5-b891-4676-fb83-aca27b8ba940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x",
    "outputId": "d592bf6b-ea84-43e6-8bc8-1b9785e48ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'NUM', 'NOUN', 'PRON', 'CONJ', 'VERB', 'X', 'ADP', '.', 'ADJ', 'ADV', 'PRT', 'DET'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "URC1B2nvPGFt",
    "outputId": "9fd9ad31-e28a-4cb3-dd6e-da9cf31fe3eb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdUklEQVR4nO3de7SldX3f8fcnM8VlkhpQJoRwcRAHFaiZyCxlJZqgiA4kSzCLKDSRwVJHl7BSqE3FJC02aoNJ7HTRKC4ME4bUcInEQF1jcIoYTSvKIISLCgyIMlNuAZUmWBH89o/9O/jM4Zy5nOvvHN6vtfY6z/N9Lvu795yz57Of5/ntnapCkiRJffmx+W5AkiRJT2dIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSerQ0vluYKbtvffetXz58vluQ5IkaaduuOGGf6iqZRMtW3Qhbfny5WzevHm+25AkSdqpJN+cbJmnOyVJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDu00pCVZn+TBJLcOapcluand7klyU6svT/K9wbKPDrY5IsktSbYkOS9JWv25STYlubP93KvV09bbkuTmJC+b+YcvSZLUp105knYRsHpYqKo3V9XKqloJXAH81WDxXWPLquodg/r5wNuAFe02ts+zgWuqagVwTZsHOHaw7tq2vSRJ0jPCTkNaVX0eeGSiZe1o2JuAS3a0jyT7As+pquuqqoCLgRPa4uOBDW16w7j6xTVyHbBn248kSdKiN93v7nwV8EBV3TmoHZTkRuBR4Peq6gvAfsDWwTpbWw1gn6q6r03fD+zTpvcD7p1gm/vQblu36Y5pbX/WMYfMUCeSJGlXTDekncz2R9HuAw6sqoeTHAH8dZLDdnVnVVVJanebSLKW0SlRDjzwwN3dXJIkqTtTHt2ZZCnwa8BlY7Wq+n5VPdymbwDuAg4BtgH7Dzbfv9UAHhg7jdl+Ptjq24ADJtlmO1V1QVWtqqpVy5Ytm+pDkiRJ6sZ0PoLjtcDXq+qp05hJliVZ0qZfwOii/7vb6cxHkxzZrmM7BbiybXYVsKZNrxlXP6WN8jwS+O7gtKgkSdKitisfwXEJ8EXgRUm2JjmtLTqJpw8Y+CXg5vaRHJ8A3lFVY4MO3gn8KbCF0RG2T7f6ucAxSe5kFPzObfWNwN1t/Y+17SVJkp4RdnpNWlWdPEn91AlqVzD6SI6J1t8MHD5B/WHg6AnqBZy+s/4kSZIWI79xQJIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSerQTkNakvVJHkxy66D23iTbktzUbscNlr0nyZYktyd5/aC+utW2JDl7UD8oyZda/bIke7T6s9r8lrZ8+Uw9aEmSpN7typG0i4DVE9TXVdXKdtsIkORQ4CTgsLbNR5IsSbIE+DBwLHAocHJbF+CDbV8vBL4NnNbqpwHfbvV1bT1JkqRnhJ2GtKr6PPDILu7veODSqvp+VX0D2AK8vN22VNXdVfU4cClwfJIArwE+0bbfAJww2NeGNv0J4Oi2viRJ0qI3nWvSzkhyczsduler7QfcO1hna6tNVn8e8J2qemJcfbt9teXfbetLkiQtelMNaecDBwMrgfuAD81YR1OQZG2SzUk2P/TQQ/PZiiRJ0oyYUkirqgeq6smq+iHwMUanMwG2AQcMVt2/1SarPwzsmWTpuPp2+2rLf6qtP1E/F1TVqqpatWzZsqk8JEmSpK5MKaQl2Xcw+0ZgbOTnVcBJbWTmQcAK4MvA9cCKNpJzD0aDC66qqgKuBU5s268Brhzsa02bPhH4bFtfkiRp0Vu6sxWSXAIcBeydZCtwDnBUkpVAAfcAbweoqtuSXA58FXgCOL2qnmz7OQO4GlgCrK+q29pdvBu4NMn7gRuBC1v9QuDPk2xhNHDhpGk/WkmSpAVipyGtqk6eoHzhBLWx9T8AfGCC+kZg4wT1u/nR6dJh/f8Bv76z/iRJkhYjv3FAkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6tBOQ1qS9UkeTHLroPZHSb6e5OYkn0yyZ6svT/K9JDe120cH2xyR5JYkW5KclySt/twkm5Lc2X7u1epp621p9/OymX/4kiRJfdqVI2kXAavH1TYBh1fVS4E7gPcMlt1VVSvb7R2D+vnA24AV7Ta2z7OBa6pqBXBNmwc4drDu2ra9JEnSM8JOQ1pVfR54ZFztM1X1RJu9Dth/R/tIsi/wnKq6rqoKuBg4oS0+HtjQpjeMq19cI9cBe7b9SJIkLXozcU3avwI+PZg/KMmNSf42yatabT9g62Cdra0GsE9V3dem7wf2GWxz7yTbSJIkLWpLp7Nxkt8FngA+3kr3AQdW1cNJjgD+Oslhu7q/qqokNYU+1jI6JcqBBx64u5tLkiR1Z8pH0pKcCvwq8BvtFCZV9f2qerhN3wDcBRwCbGP7U6L7txrAA2OnMdvPB1t9G3DAJNtsp6ouqKpVVbVq2bJlU31IkiRJ3ZhSSEuyGvj3wBuq6rFBfVmSJW36BYwu+r+7nc58NMmRbVTnKcCVbbOrgDVtes24+iltlOeRwHcHp0UlSZIWtZ2e7kxyCXAUsHeSrcA5jEZzPgvY1D5J47o2kvOXgN9P8gPgh8A7qmps0ME7GY0UfTaja9jGrmM7F7g8yWnAN4E3tfpG4DhgC/AY8NbpPFBJkqSFZKchrapOnqB84STrXgFcMcmyzcDhE9QfBo6eoF7A6TvrT5IkaTHyGwckSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUPT+u5OSQvbuk13TGv7s445ZIY6kSSN55E0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDuxTSkqxP8mCSWwe15ybZlOTO9nOvVk+S85JsSXJzkpcNtlnT1r8zyZpB/Ygkt7RtzkuSHd2HJEnSYrerR9IuAlaPq50NXFNVK4Br2jzAscCKdlsLnA+jwAWcA7wCeDlwziB0nQ+8bbDd6p3chyRJ0qK2SyGtqj4PPDKufDywoU1vAE4Y1C+ukeuAPZPsC7we2FRVj1TVt4FNwOq27DlVdV1VFXDxuH1NdB+SJEmL2nSuSdunqu5r0/cD+7Tp/YB7B+ttbbUd1bdOUN/RfWwnydokm5Nsfuihh6b4cCRJkvoxIwMH2hGwmol9TeU+quqCqlpVVauWLVs2m21IkiTNiemEtAfaqUrazwdbfRtwwGC9/VttR/X9J6jv6D4kSZIWtemEtKuAsRGaa4ArB/VT2ijPI4HvtlOWVwOvS7JXGzDwOuDqtuzRJEe2UZ2njNvXRPchSZK0qC3dlZWSXAIcBeydZCujUZrnApcnOQ34JvCmtvpG4DhgC/AY8FaAqnokyfuA69t6v19VY4MR3sloBOmzgU+3Gzu4D0mSpEVtl0JaVZ08yaKjJ1i3gNMn2c96YP0E9c3A4RPUH57oPiRJkhY7v3FAkiSpQ4Y0SZKkDhnSJEmSOrRL16RJkp451m26Y1rbn3XMITPUifTM5pE0SZKkDhnSJEmSOuTpTkmaZdM5feipQ+mZyyNpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhPydNkiQtSgv9K848kiZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHVoyiEtyYuS3DS4PZrkzCTvTbJtUD9usM17kmxJcnuS1w/qq1ttS5KzB/WDknyp1S9LssfUH6okSdLCMeWQVlW3V9XKqloJHAE8BnyyLV43tqyqNgIkORQ4CTgMWA18JMmSJEuADwPHAocCJ7d1AT7Y9vVC4NvAaVPtV5IkaSGZqdOdRwN3VdU3d7DO8cClVfX9qvoGsAV4ebttqaq7q+px4FLg+CQBXgN8om2/AThhhvqVJEnq2kyFtJOASwbzZyS5Ocn6JHu12n7AvYN1trbaZPXnAd+pqifG1SVJkha9aYe0dp3YG4C/bKXzgYOBlcB9wIemex+70MPaJJuTbH7ooYdm++4kSZJm3UwcSTsW+EpVPQBQVQ9U1ZNV9UPgY4xOZwJsAw4YbLd/q01WfxjYM8nScfWnqaoLqmpVVa1atmzZDDwkSZKk+TUTIe1kBqc6k+w7WPZG4NY2fRVwUpJnJTkIWAF8GbgeWNFGcu7B6NTpVVVVwLXAiW37NcCVM9CvJElS95bufJXJJfkJ4Bjg7YPyHyZZCRRwz9iyqrotyeXAV4EngNOr6sm2nzOAq4ElwPqquq3t693ApUneD9wIXDidfiVJkhaKaYW0qvonRhf4D2tv2cH6HwA+MEF9I7Bxgvrd/Oh0qSRJ0jOG3zggSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHVo63w1IkjRd6zbdMa3tzzrmkBnqRJo50z6SluSeJLckuSnJ5lZ7bpJNSe5sP/dq9SQ5L8mWJDcnedlgP2va+ncmWTOoH9H2v6Vtm+n2LEmS1LuZOt356qpaWVWr2vzZwDVVtQK4ps0DHAusaLe1wPkwCnXAOcArgJcD54wFu7bO2wbbrZ6hniVJkro1W9ekHQ9saNMbgBMG9Ytr5DpgzyT7Aq8HNlXVI1X1bWATsLote05VXVdVBVw82JckSdKiNRMhrYDPJLkhydpW26eq7mvT9wP7tOn9gHsH225ttR3Vt05QlyRJWtRmYuDAK6tqW5KfBjYl+fpwYVVVkpqB+5lUC4drAQ488MDZvCtJkqQ5Me0jaVW1rf18EPgko2vKHminKmk/H2yrbwMOGGy+f6vtqL7/BPXxPVxQVauqatWyZcum+5AkSZLm3bRCWpKfSPLPx6aB1wG3AlcBYyM01wBXtumrgFPaKM8jge+206JXA69LslcbMPA64Oq27NEkR7ZRnacM9iVJkrRoTfd05z7AJ9unYiwF/qKq/ibJ9cDlSU4Dvgm8qa2/ETgO2AI8BrwVoKoeSfI+4Pq23u9X1SNt+p3ARcCzgU+3myRJ0qI2rZBWVXcDPzdB/WHg6AnqBZw+yb7WA+snqG8GDp9On5IkSQuNXwslSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdWjpfDcgSdIz0bpNd0x527OOOWQGO1GvPJImSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUof8CA51y+HpkqRnMo+kSZIkdciQJkmS1CFDmiRJUocMaZIkSR2ackhLckCSa5N8NcltSf5Nq783ybYkN7XbcYNt3pNkS5Lbk7x+UF/daluSnD2oH5TkS61+WZI9ptqvJEnSQjKdI2lPAO+qqkOBI4HTkxzalq2rqpXtthGgLTsJOAxYDXwkyZIkS4APA8cChwInD/bzwbavFwLfBk6bRr+SJEkLxpRDWlXdV1VfadP/F/gasN8ONjkeuLSqvl9V3wC2AC9vty1VdXdVPQ5cChyfJMBrgE+07TcAJ0y1X0mSpIVkRq5JS7Ic+HngS610RpKbk6xPsler7QfcO9hsa6tNVn8e8J2qemJcXZIkadGbdkhL8pPAFcCZVfUocD5wMLASuA/40HTvYxd6WJtkc5LNDz300GzfnSRJ0qyb1jcOJPlnjALax6vqrwCq6oHB8o8Bn2qz24ADBpvv32pMUn8Y2DPJ0nY0bbj+dqrqAuACgFWrVtV0HpM0HX5LgiRppkxndGeAC4GvVdV/GdT3Haz2RuDWNn0VcFKSZyU5CFgBfBm4HljRRnLuwWhwwVVVVcC1wIlt+zXAlVPtV5IkaSGZzpG0XwTeAtyS5KZW+x1GozNXAgXcA7wdoKpuS3I58FVGI0NPr6onAZKcAVwNLAHWV9VtbX/vBi5N8n7gRkahUJIkadGbckirqr8DMsGijTvY5gPAByaob5xou6q6m9HoT0mSpGcUv3FAkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6tC0PidNkiQ9M0zncyDBz4KcCo+kSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdWjpfDewEK3bdMe0tj/rmENmqBNJkrRYeSRNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlD3Ye0JKuT3J5kS5Kz57sfSZKkudB1SEuyBPgwcCxwKHBykkPntytJkqTZ13VIA14ObKmqu6vqceBS4Ph57kmSJGnW9f4F6/sB9w7mtwKvmKdeJHVg3aY7prX9WcccMkOdSNLsSlXNdw+TSnIisLqq/nWbfwvwiqo6Y9x6a4G1bfZFwO1z2ujT7Q38wzz3sLvsefYttH7BnufCQusX7HmuLLSeF1q/0EfPz6+qZRMt6P1I2jbggMH8/q22naq6ALhgrpramSSbq2rVfPexO+x59i20fsGe58JC6xfsea4stJ4XWr/Qf8+9X5N2PbAiyUFJ9gBOAq6a554kSZJmXddH0qrqiSRnAFcDS4D1VXXbPLclSZI067oOaQBVtRHYON997KZuTr3uBnuefQutX7DnubDQ+gV7nisLreeF1i903nPXAwckSZKeqXq/Jk2SJOkZyZC2G5JUkg8N5v9dkve26YvaR4YM1//H9nN52/b9g2V7J/lBkj+Zqx7b/NokX2+3Lyd55WDZPUn2HswfleRTbfrUJD9M8tLB8luTLJ+hvp9MclPb518m+fEJ6v8jyZ6DbQ5L8tn2tWF3JvkPSTIX/Q72+TNJLk1yV5IbkmxMcsh0ehv/7zCDvV6b5PXjamcm+XSS77Xneex2yqCXW5LcnORvkzx/sO3Yv83fJ/lKkl+Y6Z53VZIDknwjyXPb/F5tfvl89TQmyQnt7/LFbX55e75vTPK19nd46mD9U5M81J7bryZ527w1v8BM8bme0dfg2eg1yS8n+eK47ZcmeSDJz85hz7v8Op3kS632rcHv801z+Tc56Ou29jr1riQ/1pYdleS741733jyYvj/JtsH8HnPV95Ahbfd8H/i1Kf4H+g3gVwbzvw7MxiCISXtM8qvA24FXVtWLgXcAf5HkZ3Zx31uB352xTrf3vapaWVWHA4+33sbXHwFOB0jybEYjfc+tqhcBPwf8AvDOOeqXFro+CXyuqg6uqiOA9wD7zHdvk7iE0QjpoZOAPwDuas/z2O3iwTqvrqqXAp8Dfm9QH/u3+TlGj/sPZrH3Haqqe4HzgXNb6Vzggqq6Z756GjgZ+Lv2c8xdVfXzVfUSRv8GZyZ562D5ZVW1EjgK+M9J9pmzbhe2qTzX82V3ev0CsP/wTRLwWuC2qvo/c9bxbrxOV9Ur2u/wf6T9PrfbPfPQ72HAMYy+YvKcwfIvjHvde6pP4KPAusGyx+ew76cY0nbPE4wuMjxrCts+BnwtydjnsbwZuHymGhvYUY/vBn67qv4BoKq+AmygBZ9d8CngsCQvmolGd+ALwAsnqH+R0bdQAPxL4H9V1WcAquox4Azg7MH6s93vq4EfVNVHxwpV9ffAIR30NpFPAL8y9o6wvaP9Wbb/Vo8dGT7/4z0H+PY0+5uudcCRSc4EXgn88Tz3Q5KfbL2cxtMDMgBVdTfwb4HfmmDZg8BdwPPHL9P2pvtcz6Xd7bWqfsjo/4vhuicxeuM1X3bldbob7W9pLXBGe4O9IBjSdt+Hgd9I8lNT2PZS4KQkBwBPArP1DmiyHg8DbhhX29zqu+KHwB8CvzO99iaXZCmjdzu3jKsvAY7mR5+T97THUlV3AT+Z5Dlz1O/h43voqLenqapHgC8zen5h9CJ/OVDAweMO+79qgl2sBv56MP/stu7XgT8F3jeL7e9UVf0A+G1GYe3MNj/fjgf+pqruAB5OcsQk630FePH4YpIXAC8Atsxei4vGtJ7rOTaVXp86Ep7kWcBxwBWz3ehEduN1uist+C4BfrqVXjXude/geWxvQoa03VRVjwIX8/R3YhMNkx1f+xtGh1xPAi6b+e7anU7e40433YXaXzA6WnHQVHrbgWcnuYlRaPwWcOG4+v2MTiNu2s39zla/M2E+ehue8hy+Ex9/uvMLg22uTbKN0Yvy8J372KmEFzMKcBd38A71WOA+RgG6ByczenNG+3nyJOuNf97e3H7vLwHe3gK2dmyqz/V82O1eq2ozozd6L2L0e/6lefi9mK3X6fky/nTnXfPd0Hjdf05ap/4ro3c4fzaoPQzsNTaT0QXM230fWFU9nuQG4F3AocAb5rjHrwJHAJ8d1I7gR9fGjT2Gsb4negxPZDQw4d0z3O/32nUAE9bbBapXMzo1ex6jx/JLwxXbUYd/rKpHx7LCLPYLo+ftxAnqPfQ2mSuBdUleBvx4Vd2wCxfyvhr4DvBx4D8xOgWznar6YrsOchnw4Ix2vIuSrGT0JuhI4O+SXFpV981HL62f5wKvAf5FkmL0Dr4YHeke7+eBrw3mLxv/HcWa3DSf6zk1zV7H3mS9hPk51bm7r9Ndaa/DTzJ6jXrJPLezSzySNgXt3cvljK4nGPM5Ru9+x0aAnApcO8HmHwLePdvvgCbp8Q+BDyZ5Hjz1n9qpwEfa8s8Bb2nLlgC/ycSP4SJGF61O+IWws6Fd1/VbwLvaofaPA69M8lp4aiDBeYwe43gXMTv9fhZ4VpK1Y4WMRmze3kFvE6qqf2T0b7qe3XiRr6ongDOBU9p/MtvJaITaEkZBf861I3jnMzrN+S3gj5j/a9JOBP68qp5fVcur6gBGA4iG30c8dm3gHwP/bc47XDwW0nM9nV4vYfS6/BpGb7i6MsHrdDeSLGM0GOBPagF9QKwhbeo+BDw1grKqPsXoQsob2mHfX2SCIyRVdVtVbZinHq9i9J/z/27XEX0M+M3B0Yb3AS9M8vfAjYyug/nv43faRrmcx4/O68+JqroRuBk4uaq+x+i6jt9LcjujayOuB542nH62+m1/6G8EXpvRR3DcxmiE4/3T7G0po1G6s+USRiNOhyFt/DVpE13Efl/bZmygydg1aTcxOn2/pqqenMW+d+RtwLeqauw0y0eAlyT55XnqB0ansD45rnYFo5GwB6d91AKjN1PnVdWfjd9BrzL6qJk5++iHXTDV53q2/9YmMuXfi6r6GvBPwGer6p/mquHdMXydnu9e+NFr1G3A/wQ+w+hswJjx16RNdGZkXvmNA1JH2ru9m6qqu9FR0mKTZB1wZ1V9ZKcrS/PAI2lSJ5K8gdHR2PfMdy/SYpfk08BLGV06IXXJI2mSJEkd8kiaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR36/6oVhLX6bTu9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5rWmSToIaeAo",
    "outputId": "37dcd8ad-1642-4c2a-ef55-eefeff601493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH",
    "outputId": "5380d930-349e-466e-8fff-6a692cd8b6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8XCuxEBVbOY_",
    "outputId": "fd01e928-ace4-4c7e-93e6-bb6cebf7e14a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l4XsRII5kW5x",
    "outputId": "c771643e-f42f-4a1d-8632-a2c72263842e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jbrxsZ2mehWB",
    "outputId": "834d9bbd-7e23-4825-c95f-5092ef58a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05434782608695652"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "mask = (y_batch != 0).float()\n",
    "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "total_count = mask.sum().item()\n",
    "\n",
    "correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GMUyUm1hgpe3",
    "outputId": "e404ddb0-eb02-4e95-d193-d395cf5f81b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(83.8683, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "logits = model(X_batch)\n",
    "loss = 0\n",
    "for ind, row in enumerate(logits):\n",
    "    loss += criterion(row, y_batch[ind])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = 0\n",
    "                for ind, row in enumerate(logits):\n",
    "                    loss += criterion(row, y_batch[ind])\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                mask = (y_batch != 0).float()\n",
    "                \n",
    "                cur_correct_count, cur_sum_count = ((preds == y_batch).float() * mask).sum().item(), mask.sum().item()\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa",
    "outputId": "8ff174e2-3b20-47fb-be6b-6ac928ed348c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 19.51592, Accuracy = 71.40%: 100%|██████████| 572/572 [00:11<00:00, 50.76it/s]\n",
      "[1 / 50]   Val: Loss = 9.79176, Accuracy = 85.12%: 100%|██████████| 13/13 [00:00<00:00, 67.45it/s]\n",
      "[2 / 50] Train: Loss = 6.14138, Accuracy = 90.02%: 100%|██████████| 572/572 [00:11<00:00, 51.67it/s]\n",
      "[2 / 50]   Val: Loss = 7.33860, Accuracy = 89.53%: 100%|██████████| 13/13 [00:00<00:00, 62.74it/s]\n",
      "[3 / 50] Train: Loss = 4.09976, Accuracy = 93.29%: 100%|██████████| 572/572 [00:11<00:00, 51.91it/s]\n",
      "[3 / 50]   Val: Loss = 6.64958, Accuracy = 91.00%: 100%|██████████| 13/13 [00:00<00:00, 65.02it/s]\n",
      "[4 / 50] Train: Loss = 3.10218, Accuracy = 94.89%: 100%|██████████| 572/572 [00:11<00:00, 51.53it/s]\n",
      "[4 / 50]   Val: Loss = 6.18237, Accuracy = 91.99%: 100%|██████████| 13/13 [00:00<00:00, 64.87it/s]\n",
      "[5 / 50] Train: Loss = 2.47463, Accuracy = 95.88%: 100%|██████████| 572/572 [00:11<00:00, 51.37it/s]\n",
      "[5 / 50]   Val: Loss = 6.17687, Accuracy = 92.46%: 100%|██████████| 13/13 [00:00<00:00, 65.54it/s]\n",
      "[6 / 50] Train: Loss = 2.01559, Accuracy = 96.61%: 100%|██████████| 572/572 [00:11<00:00, 50.97it/s]\n",
      "[6 / 50]   Val: Loss = 6.19967, Accuracy = 92.74%: 100%|██████████| 13/13 [00:00<00:00, 62.63it/s]\n",
      "[7 / 50] Train: Loss = 1.65730, Accuracy = 97.22%: 100%|██████████| 572/572 [00:11<00:00, 50.55it/s]\n",
      "[7 / 50]   Val: Loss = 6.21337, Accuracy = 92.92%: 100%|██████████| 13/13 [00:00<00:00, 65.87it/s]\n",
      "[8 / 50] Train: Loss = 1.36998, Accuracy = 97.69%: 100%|██████████| 572/572 [00:11<00:00, 51.63it/s]\n",
      "[8 / 50]   Val: Loss = 6.42859, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 64.81it/s]\n",
      "[9 / 50] Train: Loss = 1.13551, Accuracy = 98.09%: 100%|██████████| 572/572 [00:11<00:00, 51.63it/s]\n",
      "[9 / 50]   Val: Loss = 6.49905, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 65.84it/s]\n",
      "[10 / 50] Train: Loss = 0.93904, Accuracy = 98.43%: 100%|██████████| 572/572 [00:11<00:00, 51.73it/s]\n",
      "[10 / 50]   Val: Loss = 6.72345, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 67.54it/s]\n",
      "[11 / 50] Train: Loss = 0.77093, Accuracy = 98.74%: 100%|██████████| 572/572 [00:11<00:00, 51.35it/s]\n",
      "[11 / 50]   Val: Loss = 7.03112, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 63.44it/s]\n",
      "[12 / 50] Train: Loss = 0.63074, Accuracy = 98.98%: 100%|██████████| 572/572 [00:10<00:00, 52.02it/s]\n",
      "[12 / 50]   Val: Loss = 7.24521, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 64.00it/s]\n",
      "[13 / 50] Train: Loss = 0.50910, Accuracy = 99.20%: 100%|██████████| 572/572 [00:11<00:00, 51.71it/s]\n",
      "[13 / 50]   Val: Loss = 7.74062, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 63.75it/s]\n",
      "[14 / 50] Train: Loss = 0.41146, Accuracy = 99.36%: 100%|██████████| 572/572 [00:11<00:00, 51.22it/s]\n",
      "[14 / 50]   Val: Loss = 8.25929, Accuracy = 92.85%: 100%|██████████| 13/13 [00:00<00:00, 63.42it/s]\n",
      "[15 / 50] Train: Loss = 0.33178, Accuracy = 99.50%: 100%|██████████| 572/572 [00:11<00:00, 51.79it/s]\n",
      "[15 / 50]   Val: Loss = 8.46966, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 66.53it/s]\n",
      "[16 / 50] Train: Loss = 0.26470, Accuracy = 99.61%: 100%|██████████| 572/572 [00:11<00:00, 51.65it/s]\n",
      "[16 / 50]   Val: Loss = 8.74717, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 64.62it/s]\n",
      "[17 / 50] Train: Loss = 0.21945, Accuracy = 99.68%: 100%|██████████| 572/572 [00:11<00:00, 51.80it/s]\n",
      "[17 / 50]   Val: Loss = 9.14135, Accuracy = 92.89%: 100%|██████████| 13/13 [00:00<00:00, 65.31it/s]\n",
      "[18 / 50] Train: Loss = 0.18050, Accuracy = 99.75%: 100%|██████████| 572/572 [00:11<00:00, 51.52it/s]\n",
      "[18 / 50]   Val: Loss = 9.35920, Accuracy = 92.85%: 100%|██████████| 13/13 [00:00<00:00, 65.46it/s]\n",
      "[19 / 50] Train: Loss = 0.15914, Accuracy = 99.77%: 100%|██████████| 572/572 [00:11<00:00, 51.97it/s]\n",
      "[19 / 50]   Val: Loss = 9.73440, Accuracy = 92.84%: 100%|██████████| 13/13 [00:00<00:00, 65.50it/s]\n",
      "[20 / 50] Train: Loss = 0.13964, Accuracy = 99.80%: 100%|██████████| 572/572 [00:11<00:00, 51.94it/s]\n",
      "[20 / 50]   Val: Loss = 10.11979, Accuracy = 92.78%: 100%|██████████| 13/13 [00:00<00:00, 66.93it/s]\n",
      "[21 / 50] Train: Loss = 0.12459, Accuracy = 99.81%: 100%|██████████| 572/572 [00:11<00:00, 51.31it/s]\n",
      "[21 / 50]   Val: Loss = 10.60044, Accuracy = 92.70%: 100%|██████████| 13/13 [00:00<00:00, 62.63it/s]\n",
      "[22 / 50] Train: Loss = 0.12199, Accuracy = 99.81%: 100%|██████████| 572/572 [00:11<00:00, 51.61it/s]\n",
      "[22 / 50]   Val: Loss = 10.59140, Accuracy = 92.82%: 100%|██████████| 13/13 [00:00<00:00, 68.85it/s]\n",
      "[23 / 50] Train: Loss = 0.12039, Accuracy = 99.81%: 100%|██████████| 572/572 [00:11<00:00, 51.74it/s]\n",
      "[23 / 50]   Val: Loss = 10.89222, Accuracy = 92.74%: 100%|██████████| 13/13 [00:00<00:00, 65.39it/s]\n",
      "[24 / 50] Train: Loss = 0.11862, Accuracy = 99.81%: 100%|██████████| 572/572 [00:11<00:00, 51.84it/s]\n",
      "[24 / 50]   Val: Loss = 11.11168, Accuracy = 92.87%: 100%|██████████| 13/13 [00:00<00:00, 67.95it/s]\n",
      "[25 / 50] Train: Loss = 0.10568, Accuracy = 99.82%: 100%|██████████| 572/572 [00:11<00:00, 51.88it/s]\n",
      "[25 / 50]   Val: Loss = 11.43487, Accuracy = 92.85%: 100%|██████████| 13/13 [00:00<00:00, 63.26it/s]\n",
      "[26 / 50] Train: Loss = 0.10101, Accuracy = 99.83%: 100%|██████████| 572/572 [00:11<00:00, 51.76it/s]\n",
      "[26 / 50]   Val: Loss = 11.83783, Accuracy = 92.72%: 100%|██████████| 13/13 [00:00<00:00, 62.28it/s]\n",
      "[27 / 50] Train: Loss = 0.09856, Accuracy = 99.83%: 100%|██████████| 572/572 [00:11<00:00, 51.66it/s]\n",
      "[27 / 50]   Val: Loss = 11.92078, Accuracy = 92.77%: 100%|██████████| 13/13 [00:00<00:00, 64.91it/s]\n",
      "[28 / 50] Train: Loss = 0.09698, Accuracy = 99.83%: 100%|██████████| 572/572 [00:11<00:00, 51.52it/s]\n",
      "[28 / 50]   Val: Loss = 12.13079, Accuracy = 92.77%: 100%|██████████| 13/13 [00:00<00:00, 63.15it/s]\n",
      "[29 / 50] Train: Loss = 0.10470, Accuracy = 99.82%: 100%|██████████| 572/572 [00:11<00:00, 51.20it/s]\n",
      "[29 / 50]   Val: Loss = 12.15653, Accuracy = 92.80%: 100%|██████████| 13/13 [00:00<00:00, 65.43it/s]\n",
      "[30 / 50] Train: Loss = 0.11144, Accuracy = 99.80%: 100%|██████████| 572/572 [00:11<00:00, 51.67it/s]\n",
      "[30 / 50]   Val: Loss = 12.34038, Accuracy = 92.78%: 100%|██████████| 13/13 [00:00<00:00, 66.78it/s]\n",
      "[31 / 50] Train: Loss = 0.09110, Accuracy = 99.83%: 100%|██████████| 572/572 [00:11<00:00, 51.72it/s]\n",
      "[31 / 50]   Val: Loss = 12.60685, Accuracy = 92.88%: 100%|██████████| 13/13 [00:00<00:00, 64.86it/s]\n",
      "[32 / 50] Train: Loss = 0.08555, Accuracy = 99.84%: 100%|██████████| 572/572 [00:11<00:00, 51.45it/s]\n",
      "[32 / 50]   Val: Loss = 12.83720, Accuracy = 92.83%: 100%|██████████| 13/13 [00:00<00:00, 62.74it/s]\n",
      "[33 / 50] Train: Loss = 0.08366, Accuracy = 99.84%: 100%|██████████| 572/572 [00:11<00:00, 51.72it/s]\n",
      "[33 / 50]   Val: Loss = 12.88796, Accuracy = 92.87%: 100%|██████████| 13/13 [00:00<00:00, 63.79it/s]\n",
      "[34 / 50] Train: Loss = 0.08994, Accuracy = 99.83%: 100%|██████████| 572/572 [00:11<00:00, 51.02it/s]\n",
      "[34 / 50]   Val: Loss = 13.25400, Accuracy = 92.82%: 100%|██████████| 13/13 [00:00<00:00, 64.47it/s]\n",
      "[35 / 50] Train: Loss = 0.12150, Accuracy = 99.78%: 100%|██████████| 572/572 [00:11<00:00, 51.42it/s]\n",
      "[35 / 50]   Val: Loss = 13.36898, Accuracy = 92.79%: 100%|██████████| 13/13 [00:00<00:00, 63.38it/s]\n",
      "[36 / 50] Train: Loss = 0.09872, Accuracy = 99.82%: 100%|██████████| 572/572 [00:11<00:00, 51.38it/s]\n",
      "[36 / 50]   Val: Loss = 13.39559, Accuracy = 92.92%: 100%|██████████| 13/13 [00:00<00:00, 66.61it/s]\n",
      "[37 / 50] Train: Loss = 0.08273, Accuracy = 99.84%: 100%|██████████| 572/572 [00:11<00:00, 51.88it/s]\n",
      "[37 / 50]   Val: Loss = 13.41933, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 66.20it/s]\n",
      "[38 / 50] Train: Loss = 0.07760, Accuracy = 99.85%: 100%|██████████| 572/572 [00:10<00:00, 52.14it/s]\n",
      "[38 / 50]   Val: Loss = 13.59060, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 61.39it/s]\n",
      "[39 / 50] Train: Loss = 0.07753, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 52.22it/s]\n",
      "[39 / 50]   Val: Loss = 13.72684, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 67.09it/s]\n",
      "[40 / 50] Train: Loss = 0.07874, Accuracy = 99.84%: 100%|██████████| 572/572 [00:11<00:00, 51.91it/s]\n",
      "[40 / 50]   Val: Loss = 13.85526, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 65.79it/s]\n",
      "[41 / 50] Train: Loss = 0.07914, Accuracy = 99.85%: 100%|██████████| 572/572 [00:11<00:00, 51.85it/s]\n",
      "[41 / 50]   Val: Loss = 13.96129, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 64.09it/s]\n",
      "[42 / 50] Train: Loss = 0.09071, Accuracy = 99.82%: 100%|██████████| 572/572 [00:11<00:00, 51.95it/s]\n",
      "[42 / 50]   Val: Loss = 14.16295, Accuracy = 92.73%: 100%|██████████| 13/13 [00:00<00:00, 66.58it/s]\n",
      "[43 / 50] Train: Loss = 0.14938, Accuracy = 99.73%: 100%|██████████| 572/572 [00:11<00:00, 51.90it/s]\n",
      "[43 / 50]   Val: Loss = 14.09155, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 66.28it/s]\n",
      "[44 / 50] Train: Loss = 0.08433, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 52.35it/s]\n",
      "[44 / 50]   Val: Loss = 14.51684, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 64.50it/s]\n",
      "[45 / 50] Train: Loss = 0.07658, Accuracy = 99.85%: 100%|██████████| 572/572 [00:11<00:00, 51.71it/s]\n",
      "[45 / 50]   Val: Loss = 14.25352, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 64.86it/s]\n",
      "[46 / 50] Train: Loss = 0.07393, Accuracy = 99.85%: 100%|██████████| 572/572 [00:10<00:00, 52.12it/s]\n",
      "[46 / 50]   Val: Loss = 14.54294, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 67.74it/s]\n",
      "[47 / 50] Train: Loss = 0.07419, Accuracy = 99.85%: 100%|██████████| 572/572 [00:10<00:00, 52.21it/s]\n",
      "[47 / 50]   Val: Loss = 14.73920, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 65.33it/s]\n",
      "[48 / 50] Train: Loss = 0.07512, Accuracy = 99.85%: 100%|██████████| 572/572 [00:11<00:00, 51.94it/s]\n",
      "[48 / 50]   Val: Loss = 14.74491, Accuracy = 93.04%: 100%|██████████| 13/13 [00:00<00:00, 69.09it/s]\n",
      "[49 / 50] Train: Loss = 0.07756, Accuracy = 99.85%: 100%|██████████| 572/572 [00:11<00:00, 51.82it/s]\n",
      "[49 / 50]   Val: Loss = 14.97787, Accuracy = 92.94%: 100%|██████████| 13/13 [00:00<00:00, 66.76it/s]\n",
      "[50 / 50] Train: Loss = 0.12011, Accuracy = 99.77%: 100%|██████████| 572/572 [00:11<00:00, 51.53it/s]\n",
      "[50 / 50]   Val: Loss = 14.93861, Accuracy = 92.90%: 100%|██████████| 13/13 [00:00<00:00, 66.64it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "98wr38_rw55D",
    "outputId": "fa6c2aaf-6959-43c7-a254-2fdd6b5ded9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 93.0137123066489 %\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data, batch_size=64):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    val_accuracy = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        mask = (y_batch != 0).float()\n",
    "        \n",
    "        correct += ((pred == y_batch).float() * mask).sum().item()\n",
    "        \n",
    "        total += mask.sum().item()        \n",
    "        \n",
    "    val_accuracy = float(correct)/total\n",
    "        \n",
    "    return val_accuracy\n",
    "\n",
    "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
    "print(f'Test accuracy is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USZcr-QuvcQO"
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional = True)\n",
    "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DovtCv76vcQQ",
    "outputId": "4fe9bdc6-ca78-4203-d304-a46f0a460a30",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 21.45002, Accuracy = 88.50%: 100%|██████████| 572/572 [00:16<00:00, 33.69it/s]\n",
      "[1 / 50]   Val: Loss = 22.88703, Accuracy = 93.62%: 100%|██████████| 13/13 [00:00<00:00, 27.10it/s]\n",
      "[2 / 50] Train: Loss = 6.11321, Accuracy = 96.50%: 100%|██████████| 572/572 [00:16<00:00, 33.97it/s]\n",
      "[2 / 50]   Val: Loss = 21.94577, Accuracy = 94.68%: 100%|██████████| 13/13 [00:00<00:00, 27.70it/s]\n",
      "[3 / 50] Train: Loss = 3.81908, Accuracy = 97.54%: 100%|██████████| 572/572 [00:16<00:00, 34.33it/s]\n",
      "[3 / 50]   Val: Loss = 21.36956, Accuracy = 94.92%: 100%|██████████| 13/13 [00:00<00:00, 28.01it/s]\n",
      "[4 / 50] Train: Loss = 3.21166, Accuracy = 97.84%: 100%|██████████| 572/572 [00:16<00:00, 34.13it/s]\n",
      "[4 / 50]   Val: Loss = 20.56824, Accuracy = 94.98%: 100%|██████████| 13/13 [00:00<00:00, 29.01it/s]\n",
      "[5 / 50] Train: Loss = 2.89725, Accuracy = 98.03%: 100%|██████████| 572/572 [00:16<00:00, 34.08it/s]\n",
      "[5 / 50]   Val: Loss = 19.69584, Accuracy = 95.09%: 100%|██████████| 13/13 [00:00<00:00, 29.25it/s]\n",
      "[6 / 50] Train: Loss = 2.79148, Accuracy = 98.11%: 100%|██████████| 572/572 [00:16<00:00, 34.29it/s]\n",
      "[6 / 50]   Val: Loss = 16.23483, Accuracy = 95.19%: 100%|██████████| 13/13 [00:00<00:00, 28.28it/s]\n",
      "[7 / 50] Train: Loss = 2.62858, Accuracy = 98.24%: 100%|██████████| 572/572 [00:16<00:00, 33.91it/s]\n",
      "[7 / 50]   Val: Loss = 13.40297, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 27.10it/s]\n",
      "[8 / 50] Train: Loss = 2.39729, Accuracy = 98.36%: 100%|██████████| 572/572 [00:17<00:00, 33.52it/s]\n",
      "[8 / 50]   Val: Loss = 20.00799, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 27.32it/s]\n",
      "[9 / 50] Train: Loss = 2.26812, Accuracy = 98.43%: 100%|██████████| 572/572 [00:16<00:00, 33.90it/s]\n",
      "[9 / 50]   Val: Loss = 17.79770, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 27.12it/s]\n",
      "[10 / 50] Train: Loss = 2.11647, Accuracy = 98.52%: 100%|██████████| 572/572 [00:16<00:00, 34.04it/s]\n",
      "[10 / 50]   Val: Loss = 18.69912, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 27.79it/s]\n",
      "[11 / 50] Train: Loss = 2.09884, Accuracy = 98.54%: 100%|██████████| 572/572 [00:16<00:00, 34.20it/s]\n",
      "[11 / 50]   Val: Loss = 13.89668, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 26.54it/s]\n",
      "[12 / 50] Train: Loss = 1.99563, Accuracy = 98.65%: 100%|██████████| 572/572 [00:16<00:00, 34.23it/s]\n",
      "[12 / 50]   Val: Loss = 12.23319, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 28.97it/s]\n",
      "[13 / 50] Train: Loss = 1.74625, Accuracy = 98.71%: 100%|██████████| 572/572 [00:16<00:00, 34.28it/s]\n",
      "[13 / 50]   Val: Loss = 13.96431, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 27.06it/s]\n",
      "[14 / 50] Train: Loss = 1.67344, Accuracy = 98.78%: 100%|██████████| 572/572 [00:16<00:00, 34.19it/s]\n",
      "[14 / 50]   Val: Loss = 13.05520, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 26.98it/s]\n",
      "[15 / 50] Train: Loss = 1.62224, Accuracy = 98.79%: 100%|██████████| 572/572 [00:16<00:00, 34.42it/s]\n",
      "[15 / 50]   Val: Loss = 15.53448, Accuracy = 96.37%: 100%|██████████| 13/13 [00:00<00:00, 28.15it/s]\n",
      "[16 / 50] Train: Loss = 1.80453, Accuracy = 98.75%: 100%|██████████| 572/572 [00:17<00:00, 33.62it/s]\n",
      "[16 / 50]   Val: Loss = 15.18405, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 26.21it/s]\n",
      "[17 / 50] Train: Loss = 1.43245, Accuracy = 98.90%: 100%|██████████| 572/572 [00:17<00:00, 33.32it/s]\n",
      "[17 / 50]   Val: Loss = 13.23495, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 27.00it/s]\n",
      "[18 / 50] Train: Loss = 1.50727, Accuracy = 98.87%: 100%|██████████| 572/572 [00:16<00:00, 33.95it/s]\n",
      "[18 / 50]   Val: Loss = 14.75890, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 26.78it/s]\n",
      "[19 / 50] Train: Loss = 1.47024, Accuracy = 98.89%: 100%|██████████| 572/572 [00:16<00:00, 33.74it/s]\n",
      "[19 / 50]   Val: Loss = 12.51735, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 25.30it/s]\n",
      "[20 / 50] Train: Loss = 1.37104, Accuracy = 98.93%: 100%|██████████| 572/572 [00:17<00:00, 33.55it/s]\n",
      "[20 / 50]   Val: Loss = 13.37237, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 27.93it/s]\n",
      "[21 / 50] Train: Loss = 1.45988, Accuracy = 98.91%: 100%|██████████| 572/572 [00:17<00:00, 33.16it/s]\n",
      "[21 / 50]   Val: Loss = 14.53979, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 27.79it/s]\n",
      "[22 / 50] Train: Loss = 1.33165, Accuracy = 98.97%: 100%|██████████| 572/572 [00:17<00:00, 33.12it/s]\n",
      "[22 / 50]   Val: Loss = 12.65026, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 27.67it/s]\n",
      "[23 / 50] Train: Loss = 1.31783, Accuracy = 98.99%: 100%|██████████| 572/572 [00:17<00:00, 33.17it/s]\n",
      "[23 / 50]   Val: Loss = 11.20699, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 28.17it/s]\n",
      "[24 / 50] Train: Loss = 1.34917, Accuracy = 98.96%: 100%|██████████| 572/572 [00:17<00:00, 33.15it/s]\n",
      "[24 / 50]   Val: Loss = 15.62457, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 26.66it/s]\n",
      "[25 / 50] Train: Loss = 1.34303, Accuracy = 98.97%: 100%|██████████| 572/572 [00:17<00:00, 32.95it/s]\n",
      "[25 / 50]   Val: Loss = 15.42939, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 26.09it/s]\n",
      "[26 / 50] Train: Loss = 1.24419, Accuracy = 99.04%: 100%|██████████| 572/572 [00:17<00:00, 32.85it/s]\n",
      "[26 / 50]   Val: Loss = 11.90472, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 27.79it/s]\n",
      "[27 / 50] Train: Loss = 1.17390, Accuracy = 99.06%: 100%|██████████| 572/572 [00:17<00:00, 32.85it/s]\n",
      "[27 / 50]   Val: Loss = 13.75101, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 26.84it/s]\n",
      "[28 / 50] Train: Loss = 1.29267, Accuracy = 98.98%: 100%|██████████| 572/572 [00:17<00:00, 32.85it/s]\n",
      "[28 / 50]   Val: Loss = 14.90165, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 27.73it/s]\n",
      "[29 / 50] Train: Loss = 1.09067, Accuracy = 99.10%: 100%|██████████| 572/572 [00:16<00:00, 34.37it/s]\n",
      "[29 / 50]   Val: Loss = 13.92823, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 26.83it/s]\n",
      "[30 / 50] Train: Loss = 1.04959, Accuracy = 99.13%: 100%|██████████| 572/572 [00:16<00:00, 34.59it/s]\n",
      "[30 / 50]   Val: Loss = 15.96967, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 27.13it/s]\n",
      "[31 / 50] Train: Loss = 1.26643, Accuracy = 99.01%: 100%|██████████| 572/572 [00:16<00:00, 34.53it/s]\n",
      "[31 / 50]   Val: Loss = 17.78024, Accuracy = 95.80%: 100%|██████████| 13/13 [00:00<00:00, 27.43it/s]\n",
      "[32 / 50] Train: Loss = 1.37127, Accuracy = 98.96%: 100%|██████████| 572/572 [00:16<00:00, 34.09it/s]\n",
      "[32 / 50]   Val: Loss = 15.03529, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 27.11it/s]\n",
      "[33 / 50] Train: Loss = 1.14670, Accuracy = 99.11%: 100%|██████████| 572/572 [00:16<00:00, 34.40it/s]\n",
      "[33 / 50]   Val: Loss = 14.63306, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 29.46it/s]\n",
      "[34 / 50] Train: Loss = 1.11271, Accuracy = 99.11%: 100%|██████████| 572/572 [00:16<00:00, 34.38it/s]\n",
      "[34 / 50]   Val: Loss = 19.93085, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 27.79it/s]\n",
      "[35 / 50] Train: Loss = 1.11385, Accuracy = 99.10%: 100%|██████████| 572/572 [00:16<00:00, 34.25it/s]\n",
      "[35 / 50]   Val: Loss = 18.41454, Accuracy = 96.14%: 100%|██████████| 13/13 [00:00<00:00, 26.19it/s]\n",
      "[36 / 50] Train: Loss = 1.10761, Accuracy = 99.11%: 100%|██████████| 572/572 [00:16<00:00, 34.42it/s]\n",
      "[36 / 50]   Val: Loss = 18.35468, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 26.13it/s]\n",
      "[37 / 50] Train: Loss = 1.20832, Accuracy = 99.07%: 100%|██████████| 572/572 [00:16<00:00, 34.06it/s]\n",
      "[37 / 50]   Val: Loss = 18.60796, Accuracy = 95.98%: 100%|██████████| 13/13 [00:00<00:00, 27.67it/s]\n",
      "[38 / 50] Train: Loss = 0.97939, Accuracy = 99.17%: 100%|██████████| 572/572 [00:16<00:00, 34.42it/s]\n",
      "[38 / 50]   Val: Loss = 16.12932, Accuracy = 95.85%: 100%|██████████| 13/13 [00:00<00:00, 28.66it/s]\n",
      "[39 / 50] Train: Loss = 1.11818, Accuracy = 99.14%: 100%|██████████| 572/572 [00:16<00:00, 34.42it/s]\n",
      "[39 / 50]   Val: Loss = 21.99108, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 28.00it/s]\n",
      "[40 / 50] Train: Loss = 1.38997, Accuracy = 99.00%: 100%|██████████| 572/572 [00:16<00:00, 34.31it/s]\n",
      "[40 / 50]   Val: Loss = 17.93075, Accuracy = 95.72%: 100%|██████████| 13/13 [00:00<00:00, 26.49it/s]\n",
      "[41 / 50] Train: Loss = 1.02363, Accuracy = 99.17%: 100%|██████████| 572/572 [00:16<00:00, 34.48it/s]\n",
      "[41 / 50]   Val: Loss = 22.30804, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 27.11it/s]\n",
      "[42 / 50] Train: Loss = 5.16686, Accuracy = 97.65%: 100%|██████████| 572/572 [00:16<00:00, 34.22it/s]\n",
      "[42 / 50]   Val: Loss = 18.14793, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 28.76it/s]\n",
      "[43 / 50] Train: Loss = 3.23126, Accuracy = 98.17%: 100%|██████████| 572/572 [00:16<00:00, 34.24it/s]\n",
      "[43 / 50]   Val: Loss = 14.43404, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 27.09it/s]\n",
      "[44 / 50] Train: Loss = 1.80491, Accuracy = 98.77%: 100%|██████████| 572/572 [00:16<00:00, 34.06it/s]\n",
      "[44 / 50]   Val: Loss = 15.30662, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 27.68it/s]\n",
      "[45 / 50] Train: Loss = 1.29359, Accuracy = 99.01%: 100%|██████████| 572/572 [00:16<00:00, 34.53it/s]\n",
      "[45 / 50]   Val: Loss = 14.93945, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 29.31it/s]\n",
      "[46 / 50] Train: Loss = 1.07578, Accuracy = 99.11%: 100%|██████████| 572/572 [00:16<00:00, 34.29it/s]\n",
      "[46 / 50]   Val: Loss = 14.97331, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 27.91it/s]\n",
      "[47 / 50] Train: Loss = 1.07487, Accuracy = 99.10%: 100%|██████████| 572/572 [00:16<00:00, 34.21it/s]\n",
      "[47 / 50]   Val: Loss = 15.08191, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 29.23it/s]\n",
      "[48 / 50] Train: Loss = 1.34705, Accuracy = 98.96%: 100%|██████████| 572/572 [00:17<00:00, 33.55it/s]\n",
      "[48 / 50]   Val: Loss = 15.54927, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 25.86it/s]\n",
      "[49 / 50] Train: Loss = 1.24108, Accuracy = 99.06%: 100%|██████████| 572/572 [00:17<00:00, 33.41it/s]\n",
      "[49 / 50]   Val: Loss = 14.41612, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 27.50it/s]\n",
      "[50 / 50] Train: Loss = 1.07780, Accuracy = 99.14%: 100%|██████████| 572/572 [00:17<00:00, 33.13it/s]\n",
      "[50 / 50]   Val: Loss = 15.16931, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 27.15it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BidirectionalLSTMTagger(\n",
    "    lstm_layers_count=2,\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N6d8Rlc-vcQS",
    "outputId": "5c37559a-1c58-4d3b-99ad-18bc82338f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for biderectional LSTM is 96.98281055809743 %\n"
     ]
    }
   ],
   "source": [
    "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
    "print(f'Test accuracy for biderectional LSTM is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h",
    "outputId": "cfc93024-7055-4d13-8277-4185cfc65208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.9% 128.0/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VsCstxiO03oT",
    "outputId": "7b793956-240d-40f5-b100-7102d3dc3ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUM1uYvwvcQb"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfjC-asbvcQe"
   },
   "outputs": [],
   "source": [
    "embeddings_t = Variable(torch.from_numpy(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvmUlZM0vcQh"
   },
   "outputs": [],
   "source": [
    "embeddings_t = embeddings_t.float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding.from_pretrained(embeddings_t)\n",
    "        self._lstm = nn.LSTM(embeddings_t.shape[1], \n",
    "                             lstm_hidden_dim, \n",
    "                             num_layers=lstm_layers_count, \n",
    "                             bidirectional = True)\n",
    "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7",
    "outputId": "05ebe464-9659-47ca-e54c-0737ef7802eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 43.42425, Accuracy = 79.39%: 100%|██████████| 572/572 [00:10<00:00, 56.45it/s]\n",
      "[1 / 50]   Val: Loss = 35.74380, Accuracy = 89.57%: 100%|██████████| 13/13 [00:00<00:00, 57.46it/s]\n",
      "[2 / 50] Train: Loss = 16.70755, Accuracy = 91.81%: 100%|██████████| 572/572 [00:10<00:00, 56.06it/s]\n",
      "[2 / 50]   Val: Loss = 22.88607, Accuracy = 92.45%: 100%|██████████| 13/13 [00:00<00:00, 63.22it/s]\n",
      "[3 / 50] Train: Loss = 12.57087, Accuracy = 93.68%: 100%|██████████| 572/572 [00:10<00:00, 56.33it/s]\n",
      "[3 / 50]   Val: Loss = 19.91699, Accuracy = 93.55%: 100%|██████████| 13/13 [00:00<00:00, 63.46it/s]\n",
      "[4 / 50] Train: Loss = 10.23287, Accuracy = 94.72%: 100%|██████████| 572/572 [00:09<00:00, 57.45it/s]\n",
      "[4 / 50]   Val: Loss = 16.60140, Accuracy = 94.46%: 100%|██████████| 13/13 [00:00<00:00, 65.61it/s]\n",
      "[5 / 50] Train: Loss = 8.56771, Accuracy = 95.35%: 100%|██████████| 572/572 [00:09<00:00, 57.53it/s]\n",
      "[5 / 50]   Val: Loss = 15.35053, Accuracy = 94.90%: 100%|██████████| 13/13 [00:00<00:00, 66.96it/s]\n",
      "[6 / 50] Train: Loss = 7.45641, Accuracy = 95.83%: 100%|██████████| 572/572 [00:09<00:00, 57.50it/s]\n",
      "[6 / 50]   Val: Loss = 13.12726, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 63.99it/s]\n",
      "[7 / 50] Train: Loss = 6.44723, Accuracy = 96.21%: 100%|██████████| 572/572 [00:09<00:00, 57.67it/s]\n",
      "[7 / 50]   Val: Loss = 12.77243, Accuracy = 95.59%: 100%|██████████| 13/13 [00:00<00:00, 67.14it/s]\n",
      "[8 / 50] Train: Loss = 5.81687, Accuracy = 96.50%: 100%|██████████| 572/572 [00:09<00:00, 57.34it/s]\n",
      "[8 / 50]   Val: Loss = 12.68704, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 69.59it/s]\n",
      "[9 / 50] Train: Loss = 5.17867, Accuracy = 96.75%: 100%|██████████| 572/572 [00:09<00:00, 57.66it/s]\n",
      "[9 / 50]   Val: Loss = 11.72929, Accuracy = 95.85%: 100%|██████████| 13/13 [00:00<00:00, 68.28it/s]\n",
      "[10 / 50] Train: Loss = 4.71923, Accuracy = 96.93%: 100%|██████████| 572/572 [00:09<00:00, 57.43it/s]\n",
      "[10 / 50]   Val: Loss = 10.88559, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 63.10it/s]\n",
      "[11 / 50] Train: Loss = 4.36016, Accuracy = 97.09%: 100%|██████████| 572/572 [00:10<00:00, 56.22it/s]\n",
      "[11 / 50]   Val: Loss = 11.02866, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 61.16it/s]\n",
      "[12 / 50] Train: Loss = 4.03950, Accuracy = 97.25%: 100%|██████████| 572/572 [00:09<00:00, 57.28it/s]\n",
      "[12 / 50]   Val: Loss = 11.27940, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 63.77it/s]\n",
      "[13 / 50] Train: Loss = 3.75754, Accuracy = 97.38%: 100%|██████████| 572/572 [00:10<00:00, 55.59it/s]\n",
      "[13 / 50]   Val: Loss = 10.59910, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 64.75it/s]\n",
      "[14 / 50] Train: Loss = 3.47818, Accuracy = 97.49%: 100%|██████████| 572/572 [00:09<00:00, 57.50it/s]\n",
      "[14 / 50]   Val: Loss = 10.50886, Accuracy = 96.44%: 100%|██████████| 13/13 [00:00<00:00, 66.59it/s]\n",
      "[15 / 50] Train: Loss = 3.25040, Accuracy = 97.59%: 100%|██████████| 572/572 [00:09<00:00, 58.05it/s]\n",
      "[15 / 50]   Val: Loss = 8.90876, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 65.20it/s]\n",
      "[16 / 50] Train: Loss = 3.04521, Accuracy = 97.70%: 100%|██████████| 572/572 [00:09<00:00, 58.07it/s]\n",
      "[16 / 50]   Val: Loss = 10.52613, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 66.41it/s]\n",
      "[17 / 50] Train: Loss = 2.85680, Accuracy = 97.79%: 100%|██████████| 572/572 [00:09<00:00, 57.43it/s]\n",
      "[17 / 50]   Val: Loss = 9.39862, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 68.26it/s]\n",
      "[18 / 50] Train: Loss = 2.70741, Accuracy = 97.87%: 100%|██████████| 572/572 [00:09<00:00, 57.34it/s]\n",
      "[18 / 50]   Val: Loss = 9.89353, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 66.94it/s]\n",
      "[19 / 50] Train: Loss = 2.54458, Accuracy = 97.94%: 100%|██████████| 572/572 [00:10<00:00, 56.66it/s]\n",
      "[19 / 50]   Val: Loss = 9.90051, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 67.09it/s]\n",
      "[20 / 50] Train: Loss = 2.44560, Accuracy = 98.02%: 100%|██████████| 572/572 [00:09<00:00, 57.21it/s]\n",
      "[20 / 50]   Val: Loss = 9.84598, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 60.59it/s]\n",
      "[21 / 50] Train: Loss = 2.29880, Accuracy = 98.09%: 100%|██████████| 572/572 [00:09<00:00, 57.87it/s]\n",
      "[21 / 50]   Val: Loss = 10.96002, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 61.94it/s]\n",
      "[22 / 50] Train: Loss = 2.22237, Accuracy = 98.14%: 100%|██████████| 572/572 [00:09<00:00, 57.75it/s]\n",
      "[22 / 50]   Val: Loss = 10.11118, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 67.95it/s]\n",
      "[23 / 50] Train: Loss = 2.08696, Accuracy = 98.23%: 100%|██████████| 572/572 [00:10<00:00, 56.91it/s]\n",
      "[23 / 50]   Val: Loss = 10.32909, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 66.79it/s]\n",
      "[24 / 50] Train: Loss = 1.99218, Accuracy = 98.29%: 100%|██████████| 572/572 [00:09<00:00, 57.82it/s]\n",
      "[24 / 50]   Val: Loss = 10.26461, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 65.10it/s]\n",
      "[25 / 50] Train: Loss = 1.90047, Accuracy = 98.34%: 100%|██████████| 572/572 [00:10<00:00, 57.09it/s]\n",
      "[25 / 50]   Val: Loss = 9.99596, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 69.56it/s]\n",
      "[26 / 50] Train: Loss = 1.80172, Accuracy = 98.41%: 100%|██████████| 572/572 [00:10<00:00, 57.03it/s]\n",
      "[26 / 50]   Val: Loss = 10.17088, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 69.26it/s]\n",
      "[27 / 50] Train: Loss = 1.70977, Accuracy = 98.47%: 100%|██████████| 572/572 [00:10<00:00, 56.75it/s]\n",
      "[27 / 50]   Val: Loss = 10.87997, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 61.54it/s]\n",
      "[28 / 50] Train: Loss = 1.63453, Accuracy = 98.51%: 100%|██████████| 572/572 [00:09<00:00, 58.03it/s]\n",
      "[28 / 50]   Val: Loss = 10.42712, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 67.45it/s]\n",
      "[29 / 50] Train: Loss = 1.58759, Accuracy = 98.55%: 100%|██████████| 572/572 [00:09<00:00, 57.39it/s]\n",
      "[29 / 50]   Val: Loss = 9.62937, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 65.15it/s]\n",
      "[30 / 50] Train: Loss = 1.50874, Accuracy = 98.60%: 100%|██████████| 572/572 [00:09<00:00, 57.72it/s]\n",
      "[30 / 50]   Val: Loss = 11.39642, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 66.50it/s]\n",
      "[31 / 50] Train: Loss = 1.45892, Accuracy = 98.64%: 100%|██████████| 572/572 [00:09<00:00, 57.64it/s]\n",
      "[31 / 50]   Val: Loss = 9.56216, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 64.86it/s]\n",
      "[32 / 50] Train: Loss = 1.36076, Accuracy = 98.71%: 100%|██████████| 572/572 [00:09<00:00, 57.45it/s]\n",
      "[32 / 50]   Val: Loss = 11.32814, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 61.92it/s]\n",
      "[33 / 50] Train: Loss = 1.29965, Accuracy = 98.77%: 100%|██████████| 572/572 [00:09<00:00, 57.73it/s]\n",
      "[33 / 50]   Val: Loss = 9.90834, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 67.71it/s]\n",
      "[34 / 50] Train: Loss = 1.26787, Accuracy = 98.80%: 100%|██████████| 572/572 [00:09<00:00, 57.51it/s]\n",
      "[34 / 50]   Val: Loss = 13.23822, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 59.66it/s]\n",
      "[35 / 50] Train: Loss = 1.19026, Accuracy = 98.85%: 100%|██████████| 572/572 [00:10<00:00, 56.97it/s]\n",
      "[35 / 50]   Val: Loss = 10.06238, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 68.03it/s]\n",
      "[36 / 50] Train: Loss = 1.14031, Accuracy = 98.89%: 100%|██████████| 572/572 [00:09<00:00, 57.54it/s]\n",
      "[36 / 50]   Val: Loss = 11.89702, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 66.09it/s]\n",
      "[37 / 50] Train: Loss = 1.09272, Accuracy = 98.94%: 100%|██████████| 572/572 [00:09<00:00, 57.83it/s]\n",
      "[37 / 50]   Val: Loss = 11.78989, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 69.38it/s]\n",
      "[38 / 50] Train: Loss = 1.05487, Accuracy = 98.97%: 100%|██████████| 572/572 [00:09<00:00, 58.28it/s]\n",
      "[38 / 50]   Val: Loss = 11.62010, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 67.73it/s]\n",
      "[39 / 50] Train: Loss = 0.99238, Accuracy = 99.02%: 100%|██████████| 572/572 [00:09<00:00, 57.86it/s]\n",
      "[39 / 50]   Val: Loss = 11.34250, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 60.90it/s]\n",
      "[40 / 50] Train: Loss = 0.95629, Accuracy = 99.07%: 100%|██████████| 572/572 [00:09<00:00, 57.82it/s]\n",
      "[40 / 50]   Val: Loss = 14.03569, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 65.54it/s]\n",
      "[41 / 50] Train: Loss = 0.91496, Accuracy = 99.10%: 100%|██████████| 572/572 [00:09<00:00, 57.79it/s]\n",
      "[41 / 50]   Val: Loss = 12.18909, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 67.24it/s]\n",
      "[42 / 50] Train: Loss = 0.87315, Accuracy = 99.13%: 100%|██████████| 572/572 [00:09<00:00, 57.76it/s]\n",
      "[42 / 50]   Val: Loss = 13.88885, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 65.32it/s]\n",
      "[43 / 50] Train: Loss = 0.83160, Accuracy = 99.18%: 100%|██████████| 572/572 [00:10<00:00, 56.12it/s]\n",
      "[43 / 50]   Val: Loss = 13.52800, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 62.82it/s]\n",
      "[44 / 50] Train: Loss = 0.80043, Accuracy = 99.22%: 100%|██████████| 572/572 [00:10<00:00, 56.66it/s]\n",
      "[44 / 50]   Val: Loss = 13.99084, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 62.21it/s]\n",
      "[45 / 50] Train: Loss = 0.76441, Accuracy = 99.24%: 100%|██████████| 572/572 [00:09<00:00, 57.56it/s]\n",
      "[45 / 50]   Val: Loss = 14.58497, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 64.85it/s]\n",
      "[46 / 50] Train: Loss = 0.71670, Accuracy = 99.29%: 100%|██████████| 572/572 [00:09<00:00, 58.28it/s]\n",
      "[46 / 50]   Val: Loss = 13.90249, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 67.23it/s]\n",
      "[47 / 50] Train: Loss = 0.68851, Accuracy = 99.32%: 100%|██████████| 572/572 [00:09<00:00, 57.77it/s]\n",
      "[47 / 50]   Val: Loss = 14.33531, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 63.31it/s]\n",
      "[48 / 50] Train: Loss = 0.64982, Accuracy = 99.36%: 100%|██████████| 572/572 [00:09<00:00, 57.49it/s]\n",
      "[48 / 50]   Val: Loss = 15.04771, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 61.93it/s]\n",
      "[49 / 50] Train: Loss = 0.62989, Accuracy = 99.38%: 100%|██████████| 572/572 [00:09<00:00, 57.60it/s]\n",
      "[49 / 50]   Val: Loss = 14.94418, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 65.10it/s]\n",
      "[50 / 50] Train: Loss = 0.60064, Accuracy = 99.41%: 100%|██████████| 572/572 [00:10<00:00, 57.11it/s]\n",
      "[50 / 50]   Val: Loss = 15.22670, Accuracy = 96.61%: 100%|██████████| 13/13 [00:00<00:00, 65.71it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HPUuAPGhEGVR",
    "outputId": "17d24007-bc91-47a8-9f59-b08586b0897a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for biderectional LSTM with pretrained embeddings is 96.64473661561131 %\n"
     ]
    }
   ],
   "source": [
    "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
    "print(f'Test accuracy for biderectional LSTM with pretrained embeddings is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUvN94eFvcQt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHYHKT9VvcQw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJ5zvHD1vcQz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNNs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
